@misc{adlerOrchestrationEdge2019,
  title = {Orchestration at the {{Edge}}},
  author = {Adler, Ilan},
  year = {2019},
  month = dec,
  journal = {LinkedIn Pulse},
  urldate = {2023-01-30},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\9UQPE4JW\\orchestration-edge-ilan-adler.html}
}

@misc{albersOptimalAlgorithmsRightSizing2021,
  title = {Optimal {{Algorithms}} for {{Right-Sizing Data Centers- Extended Version}}},
  author = {Albers, Susanne and Quedenfeld, Jens},
  year = {2021},
  month = jul,
  number = {arXiv:1807.05112},
  eprint = {1807.05112},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-01-30},
  abstract = {Electricity cost is a dominant and rapidly growing expense in data centers. Unfortunately, much of the consumed energy is wasted because servers are idle for extended periods of time. We study a capacity management problem that dynamically right-sizes a data center, matching the number of active servers with the varying demand for computing capacity. We resort to a data-center optimization problem introduced by Lin, Wierman, Andrew and Thereska that, over a time horizon, minimizes a combined objective function consisting of operating cost, modeled by a sequence of convex functions, and server switching cost. All prior work addresses a continuous setting in which the number of active servers, at any time, may take a fractional value. In this paper, we investigate for the first time the discrete data-center optimization problem where the number of active servers, at any time, must be integer valued. Thereby we seek truly feasible solutions. First, we show that the offline problem can be solved in polynomial time. Second, we study the online problem and extend the algorithm \{\textbackslash em Lazy Capacity Provisioning\textbackslash/\} (LCP) by Lin et al. to the discrete setting. We prove that LCP is 3-competitive. Moreover, we show that no deterministic online algorithm can achieve a competitive ratio smaller than\textasciitilde 3. In addition, we develop a randomized online algorithm that is 2-competitive against an oblivious adversary. Moreover, we prove that 2 is a lower bound for the competitive ratio of randomized online algorithms, so our algorithm is optimal. Finally, we address the continuous setting and give a lower bound of\textasciitilde 2 on the best competitiveness of online algorithms. This matches an upper bound by Bansal et al. We prove that all lower bounds still hold in a problem variant with more restricted operating cost functions, introduced by Lin et al.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\HMCQ2UM5\\Albers and Quedenfeld - 2021 - Optimal Algorithms for Right-Sizing Data Centers- .pdf;C\:\\Users\\Wim\\Zotero\\storage\\R665LCCJ\\1807.html}
}

@article{aslanElectricityIntensityInternet2018,
  title = {Electricity {{Intensity}} of {{Internet Data Transmission}}: {{Untangling}} the {{Estimates}}},
  shorttitle = {Electricity {{Intensity}} of {{Internet Data Transmission}}},
  author = {Aslan, Joshua and Mayers, Kieren and Koomey, Jonathan G. and France, Chris},
  year = {2018},
  journal = {Journal of Industrial Ecology},
  volume = {22},
  number = {4},
  pages = {785--798},
  issn = {1530-9290},
  doi = {10.1111/jiec.12630},
  urldate = {2023-01-30},
  abstract = {In order to understand the electricity use of Internet services, it is important to have accurate estimates for the average electricity intensity of transmitting data through the Internet (measured as kilowatt-hours per gigabyte [kWh/GB]). This study identifies representative estimates for the average electricity intensity of fixed-line Internet transmission networks over time and suggests criteria for making accurate estimates in the future. Differences in system boundary, assumptions used, and year to which the data apply significantly affect such estimates. Surprisingly, methodology used is not a major source of error, as has been suggested in the past. This article derives criteria to identify accurate estimates over time and provides a new estimate of 0.06 kWh/GB for 2015. By retroactively applying our criteria to existing studies, we were able to determine that the electricity intensity of data transmission (core and fixed-line access networks) has decreased by half approximately every 2 years since 2000 (for developed countries), a rate of change comparable to that found in the efficiency of computing more generally.},
  copyright = {\textcopyright{} 2017 The Authors. Journal of Industrial Ecology, published by Wiley Periodicals, Inc., on behalf of Yale University.},
  langid = {english},
  keywords = {electricity intensity,energy,industrial ecology,information and communication technology (ICT),Internet,meta-analysis,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\FH37AMWK\\Aslan et al. - 2018 - Electricity Intensity of Internet Data Transmissio.pdf;C\:\\Users\\Wim\\Zotero\\storage\\TSJPKB5K\\jiec.html}
}

@article{avivInfrastructureCodeNext2023,
  title = {Infrastructure {{From Code}}: {{The Next Generation}} of {{Cloud Lifecycle Automation}}},
  shorttitle = {Infrastructure {{From Code}}},
  author = {Aviv, Itzhak and Gafni, Ruti and Sherman, Sofia and Aviv, Berta and Sterkin, Asher and Bega, Etzik},
  year = {2023},
  month = jan,
  journal = {IEEE Software},
  volume = {40},
  number = {1},
  pages = {42--49},
  issn = {1937-4194},
  doi = {10.1109/MS.2022.3209958},
  abstract = {We identify 14 fundamental cloud infrastructure procedures (CIPs) applicable to software development processes on the public cloud and their associated challenges. We then evaluate the capabilities of leading cloud automation technologies, such as infrastructure as code, and pinpoint their gaps in enabling the CIPs.},
  keywords = {Automation,Cloud computing,Codes,Costs,notion,Optimization,Security,Software engineering},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\RWATRXVW\\Aviv et al. - 2023 - Infrastructure From Code The Next Generation of C.pdf;C\:\\Users\\Wim\\Zotero\\storage\\HNU9SU8H\\stamp.html}
}

@article{belkhirAssessingICTGlobal2018,
  title = {Assessing {{ICT}} Global Emissions Footprint: {{Trends}} to 2040 \& Recommendations},
  shorttitle = {Assessing {{ICT}} Global Emissions Footprint},
  author = {Belkhir, Lotfi and Elmeligi, Ahmed},
  year = {2018},
  month = mar,
  journal = {Journal of Cleaner Production},
  volume = {177},
  pages = {448--463},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2017.12.239},
  urldate = {2023-05-01},
  abstract = {In light of the concerted efforts to reduce global greenhouse gas emissions (GHGE) per the so-called Paris Agreement, the Information and Communication Industry (ICT) has received little attention as a significant contributor to GHGE and if anything is often highly praised for enabling efficiencies that help reduce other industry sectors footprint. In this paper, we aim at assessing the global carbon footprint of the overall ICT industry, including the contribution from the main consumer devices, the data centers and communication networks, and compare it with the to the total worldwide GHGE. We conduct a detailed and rigorous analysis of the ICT global carbon footprint, including both the production and the operational energy of ICT devices, as well as the operational energy for the supporting ICT infrastructure. We then compare this contribution to the global 2016-level GHGE. We have found that, if unchecked, ICT GHGE relative contribution could grow from roughly 1\textendash 1.6\% in 2007 to exceed 14\% of the 2016-level worldwide GHGE by 2040, accounting for more than half of the current relative contribution of the whole transportation sector. Our study also highlights the contribution of smart phones and shows that by 2020, the footprint of smart phones alone would surpass the individual contribution of desktops, laptops and displays. Finally, we offer some actionable recommendations on how to mitigate and curb the ICT explosive GHGE footprint, through a combination of renewable energy use, tax policies, managerial actions and alternative business models.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\SESJNABZ\\Belkhir and Elmeligi - 2018 - Assessing ICT global emissions footprint Trends t.html}
}

@techreport{benqassemDigitalTechnologiesEurope2021,
  title = {Digital Technologies in {{Europe}}: An Environmental Life Cycle Approach},
  author = {Benqassem, Sofia and Bordage, Frederic and {de Montenay}, Lorraine and {Delmas-Orgelet}, Julie and Domon, Firmin and Lees Perasso, Etienne and Prunel, Damien and Vateau, Caroline},
  year = {2021},
  month = dec,
  institution = {{GreenIT.fr}},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\5QV2QAU3\\Benqassem et al. - Authors in alphabetical order.pdf;C\:\\Users\\Wim\\Zotero\\storage\\H5JKNDHV\\7390.pdf}
}

@article{bergmayrSystematicReviewCloud2018,
  title = {A {{Systematic Review}} of {{Cloud Modeling Languages}}},
  author = {Bergmayr, Alexander and Breitenb{\"u}cher, Uwe and Ferry, Nicolas and Rossini, Alessandro and Solberg, Arnor and Wimmer, Manuel and Kappel, Gerti and Leymann, Frank},
  year = {2018},
  month = feb,
  journal = {ACM Computing Surveys},
  volume = {51},
  number = {1},
  pages = {22:1--22:38},
  issn = {0360-0300},
  doi = {10.1145/3150227},
  urldate = {2023-02-08},
  abstract = {Modern cloud computing environments support a relatively high degree of automation in service provisioning, which allows cloud service customers (CSCs) to dynamically acquire services required for deploying cloud applications. Cloud modeling languages (CMLs) have been proposed to address the diversity of features provided by cloud computing environments and support different application scenarios, such as migrating existing applications to the cloud, developing new cloud applications, or optimizing them. There is, however, still much debate in the research community on what a CML is, and what aspects of a cloud application and its target cloud computing environment should be modeled by a CML. Furthermore, the distinction between CMLs on a fine-grain level exposing their modeling concepts is rarely made. In this article, we investigate the diverse features currently provided by existing CMLs. We classify and compare them according to a common framework with the goal to support CSCs in selecting the CML that fits the needs of their application scenario and setting. As a result, not only features of existing CMLs are pointed out for which extensive support is already provided but also in which existing CMLs are deficient, thereby suggesting a research agenda.},
  keywords = {Cloud computing,domain-specific languages,modeling,notion}
}

@article{bizoCarbonReductionOpportunity2019,
  title = {The Carbon Reduction Opportunity of Moving to {{Amazon Web Services}}},
  author = {Bizo, Daniel},
  year = {2019},
  journal = {AWS, October},
  keywords = {notion}
}

@article{cenciEcoFriendlyElectronicsComprehensive2022,
  title = {Eco-{{Friendly Electronics}}\textemdash{{A Comprehensive Review}}},
  author = {Cenci, Marcelo Pilotto and Scarazzato, Tatiana and Munchen, Daniel Dotto and Dartora, Paula Cristina and Veit, Hugo Marcelo and Bernardes, Andrea Moura and Dias, Pablo R.},
  year = {2022},
  journal = {Advanced Materials Technologies},
  volume = {7},
  number = {2},
  pages = {2001263},
  issn = {2365-709X},
  doi = {10.1002/admt.202001263},
  urldate = {2023-05-05},
  abstract = {Eco-friendliness is becoming an indispensable feature for electrical and electronic equipment to thrive in the competitive market. This comprehensive review is the first to define eco-friendly electronics in its multiple meanings: power saving devices, end-of-life impact attenuators, equipment whose manufacturing uses green processing, electronics that use materials that minimize environmental and health risks, designs that improve lifespan, reparability, etc. More specifically, this review discusses eco-friendly technologies and materials that are being introduced to replace the well-established ones. This is done for all material classes (metals, polymers, ceramics, and composites). Manufacturing, recycling, and final product characteristics are discussed in their various interconnected aspects. Additionally, the concept of consciously planned obsolescence is introduced to address the paradoxical relationship between durability and efficiency. The overall conclusions are that there is an important global trend to make electronics more eco-friendly. However, matching the performance and stability of well-established materials and technologies seems to be the main barrier to achieve it. These new implementations can have detrimental or beneficial net impacts on the environment. Assessing their net outcome is challenging because their impacts are frequently unknown and the current evaluation methods (and tools) are incapable of comprehensively quantifying these impacts and generating reliable verdicts.},
  langid = {english},
  keywords = {eco-friendly electronics,end-of-life,green electronics,green manufacturing,ICT sustainability,recycling,sustainable materials},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\2CTXNVX2\\Cenci et al. - 2022 - Eco-Friendly Electronics—A Comprehensive Review.pdf}
}

@article{chenEfficientMultiUserComputation2016,
  title = {Efficient {{Multi-User Computation Offloading}} for {{Mobile-Edge Cloud Computing}}},
  author = {Chen, Xu and Jiao, Lei and Li, Wenzhong and Fu, Xiaoming},
  year = {2016},
  month = oct,
  journal = {IEEE/ACM Transactions on Networking},
  volume = {24},
  number = {5},
  pages = {2795--2808},
  issn = {1558-2566},
  doi = {10.1109/TNET.2015.2487344},
  abstract = {Mobile-edge cloud computing is a new paradigm to provide cloud computing capabilities at the edge of pervasive radio access networks in close proximity to mobile users. In this paper, we first study the multi-user computation offloading problem for mobile-edge cloud computing in a multi-channel wireless interference environment. We show that it is NP-hard to compute a centralized optimal solution, and hence adopt a game theoretic approach for achieving efficient computation offloading in a distributed manner. We formulate the distributed computation offloading decision making problem among mobile device users as a multi-user computation offloading game. We analyze the structural property of the game and show that the game admits a Nash equilibrium and possesses the finite improvement property. We then design a distributed computation offloading algorithm that can achieve a Nash equilibrium, derive the upper bound of the convergence time, and quantify its efficiency ratio over the centralized optimal solutions in terms of two important performance metrics. We further extend our study to the scenario of multi-user computation offloading in the multi-channel wireless contention environment. Numerical results corroborate that the proposed algorithm can achieve superior computation offloading performance and scale well as the user size increases.},
  keywords = {Cloud computing,Computation offloading,Computational modeling,Decision making,game theory,Games,Mobile communication,Mobile handsets,mobile-edge cloud computing,Nash equilibrium,notion,Wireless communication},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\JBWURUFG\\7307234.html}
}

@misc{cosnerAzureIoTReference,
  title = {Azure {{IoT}} Reference Architecture - {{Azure Reference Architectures}}},
  author = {Cosner, Matthew},
  urldate = {2023-01-30},
  abstract = {Recommended architecture for IoT applications on Azure using PaaS (platform-as-a-service) components.},
  howpublished = {https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/iot},
  langid = {american},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\4EN3GB92\\iot.html}
}

@inproceedings{costaMpOSMultiplatformOffloading2015,
  title = {{{MpOS}}: A Multiplatform Offloading System},
  shorttitle = {{{MpOS}}},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Costa, Philipp B. and Rego, Paulo A. L. and Rocha, Lincoln S. and Trinta, Fernando A. M. and {de Souza}, Jos{\'e} N.},
  year = {2015},
  month = apr,
  series = {{{SAC}} '15},
  pages = {577--584},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2695664.2695945},
  urldate = {2023-03-01},
  abstract = {Mobile applications and services have changed different aspects of modern life, besides allowing to be accessed by mobile devices at any moment, regardless of time and place. These devices usually interact with more powerful machines usually hosted at Internet on public clouds. This paper presents MpOS (Multiplatform Offloading System), a framework that supports a method-based offloading technique for applications of different mobile platforms, and was also developed initially for Android and Windows Phone. The framework provides several services to support the offloading process (e.g., discovery service, deployment service, and network profiler). In order to assess the proposed framework, we first developed an application using the MpOS framework for both Android and Windows Phone platforms. Finally, we performed an experiment to evaluate the developed applications performance. The result shows that offloading operation achieved a speedup of 14x running on cloudlet when compared with the execution on the mobile device.},
  isbn = {978-1-4503-3196-8},
  keywords = {mobile cloud computing,multiple platforms,notion,offloading}
}

@article{DataAge2025,
  title = {Data {{Age}} 2025: {{The Digitization}} of the {{World}} from {{Edge}} to {{Core}}}
}

@misc{ecopingWhyTreePlanting2021,
  title = {Why Tree Planting Alone Won't Save the World},
  author = {Ecoping},
  year = {2021},
  month = nov,
  urldate = {2023-01-30},
  abstract = {A carbon neutral calculator to calculate how to offset your website's emissions},
  howpublished = {https://ecoping.earth/blog/why-tree-planting-alone-wont-save-the-world},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\L35FHKIV\\why-tree-planting-alone-wont-save-the-world.html}
}

@misc{EnergyAwareOffloading,
  title = {Energy {{Aware Offloading}} for {{Competing Users}} on a {{Shared Communication Channel}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  urldate = {2023-03-01},
  howpublished = {https://ieeexplore.ieee.org/document/7425262},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\H2H7L3N3\\7425262.html}
}

@misc{gill2018edge,
  title = {The Edge Completes the Cloud: {{A}} Gartner Trend Insight Report},
  author = {Gill, Bob and Smith, David},
  year = {2018},
  publisher = {{Gartner}},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\TTU5FICY\\Gill and Smith - 2018 - The edge completes the cloud A gartner trend insi.pdf}
}

@misc{googleOurCommitmentSustainability2023,
  title = {Our {{Commitment}} to {{Sustainability}}},
  author = {Google},
  year = {2023},
  journal = {Google Sustainability},
  urldate = {2023-01-30},
  abstract = {Learn more about Google's sustainability plans and environmental initiatives aimed at creating a sustainable future and healthier planet for everyone.},
  howpublished = {https://sustainability.google/commitments/},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\YX7S4R9W\\commitments.html}
}

@article{guptaIFogSimToolkitModeling2017,
  title = {{{iFogSim}}: {{A}} Toolkit for Modeling and Simulation of Resource Management Techniques in the {{Internet}} of {{Things}}, {{Edge}} and {{Fog}} Computing Environments},
  shorttitle = {{{iFogSim}}},
  author = {Gupta, Harshit and Vahid Dastjerdi, Amir and Ghosh, Soumya K. and Buyya, Rajkumar},
  year = {2017},
  journal = {Software: Practice and Experience},
  volume = {47},
  number = {9},
  pages = {1275--1296},
  issn = {1097-024X},
  doi = {10.1002/spe.2509},
  urldate = {2023-03-01},
  abstract = {Internet of Things (IoT) aims to bring every object (eg, smart cameras, wearable, environmental sensors, home appliances, and vehicles) online, hence generating massive volume of data that can overwhelm storage systems and data analytics applications. Cloud computing offers services at the infrastructure level that can scale to IoT storage and processing requirements. However, there are applications such as health monitoring and emergency response that require low latency, and delay that is caused by transferring data to the cloud and then back to the application can seriously impact their performances. To overcome this limitation, Fog computing paradigm has been proposed, where cloud services are extended to the edge of the network to decrease the latency and network congestion. To realize the full potential of Fog and IoT paradigms for real-time analytics, several challenges need to be addressed. The first and most critical problem is designing resource management techniques that determine which modules of analytics applications are pushed to each edge device to minimize the latency and maximize the throughput. To this end, we need an evaluation platform that enables the quantification of performance of resource management policies on an IoT or Fog computing infrastructure in a repeatable manner. In this paper we propose a simulator, called iFogSim, to model IoT and Fog environments and measure the impact of resource management techniques in latency, network congestion, energy consumption, and cost. We describe two case studies to demonstrate modeling of an IoT environment and comparison of resource management policies. Moreover, scalability of the simulation toolkit of RAM consumption and execution time is verified under different circumstances.},
  langid = {english},
  keywords = {Edge computing,Fog computing,Internet of Things (IoT),modeling and simulation,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\IQWAY8IQ\\Gupta et al. - 2017 - iFogSim A toolkit for modeling and simulation of .pdf;C\:\\Users\\Wim\\Zotero\\storage\\XULS67IZ\\spe.html}
}

@misc{HoeDuurzaamZijn2023,
  title = {{Hoe duurzaam zijn datacenters?}},
  year = {2023},
  journal = {Centric Insights},
  urldate = {2023-01-30},
  abstract = {Vanwege het toenemend internetgebruik en de groeiende honger naar data zijn meer datacenters nodig. Maar hoe duurzaam zijn die? En waar moeten die gebouwd worden? Voor- en tegenstanders aan het woord.},
  howpublished = {https://insights.centric.eu/nl/themes/cloud/hoe-duurzaam-zijn-datacenters/},
  langid = {dutch},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\MU5DEB2S\\hoe-duurzaam-zijn-datacenters.html}
}

@misc{https://www.bcg.com/about/people/experts/jan-hinnerk-mohrChipMakersDecarbonization2023,
  title = {For {{Chip Makers}}, the {{Decarbonization Challenge Lies Upstream}}},
  author = {{https://www.bcg.com/about/people/experts/jan-hinnerk-mohr}},
  year = {2023},
  month = may,
  journal = {BCG Global},
  urldate = {2023-06-16},
  abstract = {To slash their substantial greenhouse gas emissions, chip manufacturers must focus on turning their upstream supply chain green.},
  howpublished = {https://www.bcg.com/publications/2023/why-chip-makers-need-to-focus-on-the-upcoming-decarbonization-challenges},
  langid = {english},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\4Z7XG6CG\\why-chip-makers-need-to-focus-on-the-upcoming-decarbonization-challenges.html}
}

@misc{https://www.bcg.com/about/people/experts/jan-hinnerk-mohrChipmakersDecarbonizationChallenge2023,
  title = {For {{Chipmakers}}, the {{Decarbonization Challenge Lies Upstream}}},
  author = {{https://www.bcg.com/about/people/experts/jan-hinnerk-mohr}},
  year = {2023},
  month = may,
  journal = {BCG Global},
  urldate = {2023-05-24},
  abstract = {To slash their substantial greenhouse gas emissions, chip manufacturers must focus on turning their upstream supply chain green.},
  howpublished = {https://www.bcg.com/publications/2023/why-chipmakers-need-to-focus-on-the-upcoming-decarbonization-challenges},
  langid = {english},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\PM4VTFC7\\why-chipmakers-need-to-focus-on-the-upcoming-decarbonization-challenges.html}
}

@misc{huEdgeCentralCloud2019,
  title = {Edge and {{Central Cloud Computing}}: {{A Perfect Pairing}} for {{High Energy Efficiency}} and {{Low-latency}}},
  shorttitle = {Edge and {{Central Cloud Computing}}},
  author = {Hu, Xiaoyan and Wang, Lifeng and Wong, Kai-Kit and Zhang, Yangyang and Zheng, Zhongbin and Tao, Meixia},
  year = {2019},
  month = oct,
  number = {arXiv:1806.08943},
  eprint = {1806.08943},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-01-31},
  abstract = {In this paper, we study the coexistence and synergy between edge and central cloud computing in a heterogeneous cellular network (HetNet), which contains a multi-antenna macro base station (MBS), multiple multi-antenna small base stations (SBSs) and multiple single-antenna user equipment (UEs). The SBSs are empowered by edge clouds offering limited computing services for UEs, whereas the MBS provides high-performance central cloud computing services to UEs via a restricted multiple-input multiple-output (MIMO) backhaul to their associated SBSs. With processing latency constraints at the central and edge networks, we aim to minimize the system energy consumption used for task offloading and computation. The problem is formulated by jointly optimizing the cloud selection, the UEs' transmit powers, the SBSs' receive beamformers, and the SBSs' transmit covariance matrices, which is \{a mixed-integer and non-convex optimization problem\}. Based on methods such as decomposition approach and successive pseudoconvex approach, a tractable solution is proposed via an iterative algorithm. The simulation results show that our proposed solution can achieve great performance gain over conventional schemes using edge or central cloud alone. Also, with large-scale antennas at the MBS, the massive MIMO backhaul can significantly reduce the complexity of the proposed algorithm and obtain even better performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Networking and Internet Architecture,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\PUMXD25Z\\Hu et al. - 2019 - Edge and Central Cloud Computing A Perfect Pairin.pdf;C\:\\Users\\Wim\\Zotero\\storage\\HM7YH3W5\\1806.html}
}

@inproceedings{hungSchedulingJobsGeodistributed2015,
  title = {Scheduling Jobs across Geo-Distributed Datacenters},
  booktitle = {Proceedings of the {{Sixth ACM Symposium}} on {{Cloud Computing}}},
  author = {Hung, Chien-Chun and Golubchik, Leana and Yu, Minlan},
  year = {2015},
  month = aug,
  series = {{{SoCC}} '15},
  pages = {111--124},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2806777.2806780},
  urldate = {2023-01-30},
  abstract = {With growing data volumes generated and stored across geo-distributed datacenters, it is becoming increasingly inefficient to aggregate all data required for computation at a single datacenter. Instead, a recent trend is to distribute computation to take advantage of data locality, thus reducing the resource (e.g., bandwidth) costs while improving performance. In this trend, new challenges are emerging in job scheduling, which requires coordination among the datacenters as each job runs across geo-distributed sites. In this paper, we propose novel job scheduling algorithms that coordinate job scheduling across datacenters with low overhead, while achieving near-optimal performance. Our extensive simulation study with realistic job traces shows that the proposed scheduling algorithms result in up to 50\% improvement in average job completion time over the Shortest Remaining Processing Time (SRPT) based approaches.},
  isbn = {978-1-4503-3651-2},
  keywords = {notion}
}

@article{iorgaFogComputingConceptual2018,
  title = {Fog {{Computing Conceptual Model}}},
  author = {Iorga, Michaela and Feldman, Larry and Barton, Robert and Martin, Michael J. and Goren, Nedim S. and Mahmoudi, Charif},
  year = {2018},
  month = mar,
  journal = {NIST},
  publisher = {{Michaela Iorga, Larry Feldman, Robert Barton, Michael J. Martin, Nedim S. Goren, Charif Mahmoudi}},
  urldate = {2023-02-01},
  abstract = {Managing the data generated by Internet of Things (IoT) sensors and actuators is one of the biggest challenges faced when deploying an IoT system.},
  langid = {english},
  keywords = {notion},
  annotation = {Last Modified: 2018-11-10T10:11-05:00},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\NXYQ4HH9\\Iorga et al. - 2018 - Fog Computing Conceptual Model.pdf}
}

@misc{IoTDeviceDesign2021,
  title = {{{IoT Device Design}}},
  year = {aug-2021},
  publisher = {{Eseye}},
  urldate = {2023-01-30},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\UNWEZAC7\\2021 - IoT Device Design.pdf}
}

@misc{IoTEcosystem2022,
  title = {The {{IoT}} Ecosystem in 2022: Components, Industry Alliances and Legal Environments},
  shorttitle = {The {{IoT}} Ecosystem in 2022},
  journal = {Thales Group},
  urldate = {2023-01-30},
  abstract = {Thales explains the seven key characteristics that determine the make-up of every successful IoT product and talks through the confusing jargon that IoT creators will need to know in order to operate successfully in this market.},
  howpublished = {https://www.thalesgroup.com/en/markets/digital-identity-and-security/iot/inspired/iot-building-blocks},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\5J77IGD2\\iot-building-blocks.html}
}

@misc{IoTSolutionArchitecture2022,
  title = {{{IoT Solution Architecture}}: {{Components}} and {{Design Tips}}},
  shorttitle = {{{IoT Solution Architecture}}},
  year = {2022},
  month = aug,
  journal = {ITRex},
  urldate = {2023-01-30},
  abstract = {Explore the essential components of an IoT architecture, learn what devising one may look like in practice, and find out why a thought-out Internet of Things architecture is the cornerstone of a successful IoT project.},
  langid = {american},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\MRS2RF4V\\iot-architecture-components-design-tips.html}
}

@article{jiangComputationOffloadingEdge2019,
  title = {Toward {{Computation Offloading}} in {{Edge Computing}}: {{A Survey}}},
  shorttitle = {Toward {{Computation Offloading}} in {{Edge Computing}}},
  author = {Jiang, Congfeng and Cheng, Xiaolan and Gao, Honghao and Zhou, Xin and Wan, Jian},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {131543--131558},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2938660},
  abstract = {The explosive growth of massive data generation from Internet of Things in industrial, agricultural and scientific communities has led to a rapid increase for data analytics in cloud data centers. The ubiquitous and pervasive demand for near-data processing urges the edge computing paradigm in recent years. Edge computing is promising for less network backbone bandwidth usage and thus less data center side processing pressure, as well as enhanced service responsiveness and data privacy protection. Computation offloading plays a crucial role in edge computing in terms of network packets transmission and system responsiveness through dynamic task partitioning between cloud data centers and edge servers and edge devices. In this paper a thorough literature review is conducted to reveal the state-of-the-art of computation offloading in edge computing. Various aspects of computation offloading, including energy consumption minimization, Quality of Services guarantee, and Quality of Experiences enhancement are surveyed. Moreover, resource scheduling approaches, gaming and tradeoffing among system performance and overheads for computation offloading decision making are also reviewed.},
  keywords = {Cloud computing,computation offloading,Computational modeling,Computer architecture,Data centers,Edge computing,edge-cloud collaboration,game theory,Internet of Things,notion,Task analysis,task partitioning},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\2V5ENDFJ\\Jiang et al. - 2019 - Toward Computation Offloading in Edge Computing A.pdf;C\:\\Users\\Wim\\Zotero\\storage\\ZBNGPY5Q\\8821273.html}
}

@article{jiangEnergyAwareEdge2020,
  title = {Energy Aware Edge Computing: {{A}} Survey},
  shorttitle = {Energy Aware Edge Computing},
  author = {Jiang, Congfeng and Fan, Tiantian and Gao, Honghao and Shi, Weisong and Liu, Liangkai and C{\'e}rin, Christophe and Wan, Jian},
  year = {2020},
  month = feb,
  journal = {Computer Communications},
  volume = {151},
  pages = {556--580},
  issn = {0140-3664},
  doi = {10.1016/j.comcom.2020.01.004},
  urldate = {2023-01-31},
  abstract = {Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.},
  langid = {english},
  keywords = {Benchmarking,Computation partitioning,Computing offloading,Edge computing,Energy efficiency,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\UJSHDZAQ\\Jiang et al. - 2020 - Energy aware edge computing A survey.pdf;C\:\\Users\\Wim\\Zotero\\storage\\BAQQ6NGN\\S014036641930831X.html}
}

@inproceedings{jiangEnergyProportionalServers2017,
  title = {Energy {{Proportional Servers}}: {{Where Are We}} in 2016?},
  shorttitle = {Energy {{Proportional Servers}}},
  booktitle = {2017 {{IEEE}} 37th {{International Conference}} on {{Distributed Computing Systems}} ({{ICDCS}})},
  author = {Jiang, Congfeng and Wang, Yumei and Ou, Dongyang and Luo, Bing and Shi, Weisong},
  year = {2017},
  month = jun,
  pages = {1649--1660},
  issn = {1063-6927},
  doi = {10.1109/ICDCS.2017.285},
  abstract = {The huge energy consumption in data centers produces not only high electricity bill but also tremendous carbon footprints. Although today's servers and data centers of leading internet companies are more energy efficient than ever before, the fluctuations in external workload and internal resource utilization calls for energy proportional computing. Insight into server energy proportionality can help improve workload placement while also reducing energy consumption. In this paper, we investigate all 477 valid published results of SPECpower\_ssj benchmark from 2007 to 2016Q3 and reorganize them by hardware availability year for more accurate analysis on production servers. Through comprehensive analysis we find that: (1) The specious stagnation of energy proportionality in recent years is mainly caused by the adoption of processors of specific microarchitecture and is not the indicative trend of energy proportionality improvement. (2) Microarchitecture evolution has more influence on energy efficiency improvement than energy proportionality. (3) Today's servers' peak energy efficiencies are shifting from 100\% resource utilization to 80\% or 70\% utilization and server energy proportionality improves with such shifting. We then conduct extensive experiments on 4 rack servers to investigate the energy efficiency variations under different hardware configurations, including memory per core installation and processor frequency scaling. Our experiments show that hardware configuration has significant impact on server's energy efficiency. Our findings presented in this paper provide useful insights and guidance to system designers, as well as data center operators for energy proportionality aware workload placement and energy savings.},
  keywords = {Benchmark testing,Data Centers,Energy consumption,Energy efficiency,Energy Efficiency,Energy Proportionality,Hardware,Market research,Measurement,notion,Servers,SPECpower\_ssj},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\9UFGGD3T\\7980102.html}
}

@misc{JobCharacteristicsLargeScale,
  title = {Job\_{{Characteristics}}\_on\_{{Large-Scale}}\_{{Systems}}\_{{Long-Term}}\_{{Analysis}}\_{{Quantification}}\_and\_{{Implications}}.Pdf},
  journal = {Google Docs},
  urldate = {2023-05-04},
  howpublished = {https://drive.google.com/file/d/1lRAGadltkrUhh2KV1lQxrOt\_nYrGGfdF/view?usp=embed\_facebook},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\C6JV9K7L\\view.html}
}

@article{kampYouReDoing2010,
  title = {You're {{Doing It Wrong}}: {{Think}} You've Mastered the Art of Server Performance? {{Think}} Again.},
  shorttitle = {You're {{Doing It Wrong}}},
  author = {Kamp, Poul-Henning},
  year = {2010},
  month = jun,
  journal = {Queue},
  volume = {8},
  number = {6},
  pages = {20--27},
  issn = {1542-7730},
  doi = {10.1145/1810226.1814327},
  urldate = {2023-01-30},
  abstract = {Would you believe me if I claimed that an algorithm that has been on the books as "optimal" for 46 years, which has been analyzed in excruciating detail by geniuses like Knuth and taught in all computer science courses in the world, can be optimized to run 10 times faster? A couple of years ago, I fell into some interesting company and became the author of an open source HTTP accelerator called Varnish, basically an HTTP cache to put in front of slow Web servers. Today Varnish is used by Web sites of all sorts, from Facebook, Wikia, and Slashdot to obscure sites you have surely never heard of.},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\AAL5GTEN\\Kamp - 2010 - You’re Doing It Wrong Think you’ve mastered the a.pdf}
}

@article{koLivePrefetchingMobile2017,
  title = {Live {{Prefetching}} for {{Mobile Computation Offloading}}},
  author = {Ko, Seung-Woo and Huang, Kaibin and Kim, Seong-Lyun and Chae, Hyukjin},
  year = {2017},
  month = may,
  journal = {IEEE Transactions on Wireless Communications},
  volume = {16},
  number = {5},
  pages = {3057--3071},
  issn = {1558-2248},
  doi = {10.1109/TWC.2017.2674665},
  abstract = {Mobile computation offloading refers to techniques for offloading computation intensive tasks from mobile devices to the cloud so as to lengthen the formers' battery lives and enrich their features. The conventional designs fetch (transfer) user-specific data from mobiles to the cloud prior to computing, called offline prefetching. However, this approach can potentially result in excessive fetching of large volumes of data and cause heavy loads on radio-access networks. To solve this problem, the novel technique of live prefetching, which seamlessly integrates the task-level computation prediction and prefetching within the cloud-computing process of a large program with numerous tasks, is proposed in this paper. The technique avoids excessive fetching but retains the feature of leveraging prediction to reduce the program runtime and mobile transmission energy. By modeling the tasks in an offloaded program as a stochastic sequence, stochastic optimization is applied to design fetching policies to minimize mobile energy consumption under a deadline constraint. The policies enable real-time control of the prefetched-data sizes of candidates for future tasks. For slow fading, the optimal policy is derived and shown to have a threshold-based structure, selecting candidate tasks for prefetching and controlling their prefetched data based on their likelihoods. The result is extended to design close-to-optimal prefetching policies to fast fading channels. Compared with fetching without prediction, live prefetching is shown theoretically to always achieve reduction on mobile energy consumption.},
  keywords = {Cloud computing,Computational modeling,Energy consumption,Fading channels,Live prefetching,Mobile communication,notion,Prefetching,prefetching gain,stochastic optimization,task-level prediction,threshold-based structure,Wireless communication},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\BFQ49UEL\\Ko et al. - 2017 - Live Prefetching for Mobile Computation Offloading.pdf;C\:\\Users\\Wim\\Zotero\\storage\\7CP5UMED\\7880703.html}
}

@article{kotsiouLDSFLowLatencyDistributed2020,
  title = {{{LDSF}}: {{Low-Latency Distributed Scheduling Function}} for {{Industrial Internet}} of {{Things}}},
  shorttitle = {{{LDSF}}},
  author = {Kotsiou, Vasileios and Papadopoulos, Georgios Z. and Chatzimisios, Periklis and Theoleyre, Fabrice},
  year = {2020},
  month = sep,
  journal = {IEEE Internet of Things Journal},
  volume = {7},
  number = {9},
  pages = {8688--8699},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2020.2995499},
  abstract = {The Industrial Internet of Things (IIoT) is expected to be a key enabler for the Industry 4.0. However, networked control automation often requires high reliability and bounded latency to react properly. Thus, modern wireless protocols for industrial networks, such as IEEE 802.15.4-2015 time-slotted channel hopping (TSCH), rely on a strict schedule of the transmissions to avoid collisions and to make the end-to-end traffic deterministic. Unfortunately, guaranteeing a bounded end-to-end latency is particularly challenging since transmissions have to be temporally chained. Even worse, potential degradation of the link quality may result in reconstructing the whole TSCH schedule along the path. In this article, we propose the low-latency distributed scheduling function (LDSF) that relies on the organization of the slotframe in smaller parts, called blocks. Each transmitter selects the right set of blocks, depending on its hop distance from the border router, so that retransmission opportunities are automatically scheduled. To save energy, a node can still turn off its radio as soon as its packet is correctly acknowledged. Our mathematical analysis as well as our simulation evaluation show the efficiency of the proposed LDSF algorithm compared to three state-of-the-art scheduling functions (SFs): 1) the minimal SF (MSF); 2) low-latency SF (LLSF); and 3) stratum.},
  keywords = {Delays,Distributed scheduling,end-to-end delay,high reliability,Industrial Internet of Things (IIoT),Internet of Things,Job shop scheduling,notion,Reliability,Schedules,Scheduling algorithms},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\F5L5K9JG\\9096372.html}
}

@article{kuAnticipatingCriticalMaterials2018,
  title = {Anticipating Critical Materials Implications from the {{Internet}} of {{Things}} ({{IOT}}): {{Potential}} Stress on Future Supply Chains from Emerging Data Storage Technologies},
  shorttitle = {Anticipating Critical Materials Implications from the {{Internet}} of {{Things}} ({{IOT}})},
  author = {Ku, Anthony Y.},
  year = {2018},
  month = apr,
  journal = {Sustainable Materials and Technologies},
  volume = {15},
  pages = {27--32},
  issn = {2214-9937},
  doi = {10.1016/j.susmat.2017.10.001},
  urldate = {2023-05-05},
  abstract = {Over the past decade, raw material price spikes have called attention to the supply security of a variety of critical materials, including rhenium, rare earth elements, and helium. While market forces play an important role in creating and resolving these situations, transitions in technology also create step-changes in demand that increase or decrease the criticality of different materials. With an appropriate understanding of how materials are used in various applications, it is possible to explore the critical materials implications associated with the introduction of new technologies. Work is already underway to investigate and mitigate the materials impacts of emerging clean energy technologies related to solar power and energy storage. Rapid technological change is also being enabled by information technologies and the Internet of Things (IOT). Here, less work has been done around materials trends and their implications. This paper presents a case study around emerging technologies for data storage and what their implementation at mass scale (zettabyte, ZB) might mean for existing supply chains and market dynamics for certain critical materials.},
  langid = {english},
  keywords = {Critical materials,Data,Memory,Zettabyte},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\P64VH68Y\\Ku - 2018 - Anticipating critical materials implications from .pdf}
}

@inproceedings{liEveryLiteLightweightScripting2018,
  title = {{{EveryLite}}: {{A Lightweight Scripting Language}} for {{Micro Tasks}} in {{IoT Systems}}},
  shorttitle = {{{EveryLite}}},
  booktitle = {2018 {{IEEE}}/{{ACM Symposium}} on {{Edge Computing}} ({{SEC}})},
  author = {Li, Zhenying and Peng, Xiaohui and Chao, Lu and Xu, Zhiwei},
  year = {2018},
  month = oct,
  pages = {381--386},
  doi = {10.1109/SEC.2018.00050},
  abstract = {Processing the computational tasks on the devices at the edge can significantly reduce computing load, network transmission load, and response latency. However, programming on these devices is difficult due to the resource-constrained and diversity features. This paper presents a lightweight scripting language, called EveryLite, to address this issue. EveryLite features a new @-expression to access the resources on connected devices via the REST Web interfaces and focuses on the micro tasks with limited complexity in Internet of Things (IoT) systems. We design an elastic runtime environment with a core of 37 KB and some extending modules to address the IoT devices' diversity problem. Experimental results show that the applications developed by EveryLite can be run on heterogeneous devices without modification and consume less memory than those developed by other scripting languages such as Lua and Python.},
  keywords = {computation offloading,Memory management,micro tasks,notion,Operating systems,Programming,resource constrained,Runtime environment,Scripting language,Task analysis,Web of Things},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\7VF4I5EJ\\8567695.html}
}

@article{linTimeandEnergyAwareComputationOffloading2015,
  title = {Time-and-{{Energy-Aware Computation Offloading}} in {{Handheld Devices}} to {{Coprocessors}} and {{Clouds}}},
  author = {Lin, Ying-Dar and Chu, Edward T.-H. and Lai, Yuan-Cheng and Huang, Ting-Jun},
  year = {2015},
  month = jun,
  journal = {IEEE Systems Journal},
  volume = {9},
  number = {2},
  pages = {393--405},
  issn = {1937-9234},
  doi = {10.1109/JSYST.2013.2289556},
  abstract = {Running sophisticated software on smart phones could result in poor performance and shortened battery lifetime because of their limited resources. Recently, offloading computation workload to the cloud has become a promising solution to enhance both performance and battery life of smart phones. However, it also consumes both time and energy to upload data or programs to the cloud and retrieve the results from the cloud. In this paper, we develop an offloading framework, named Ternary Decision Maker (TDM), which aims to shorten response time and reduce energy consumption at the same time. Unlike previous works, our targets of execution include an on-board CPU, an on-board GPU, and a cloud, all of which combined provide a more flexible execution environment for mobile applications. We conducted a real-world application, i.e., matrix multiplication, in order to evaluate the performance of TDM. According to our experimental results, TDM has less false offloading decision rate than existing methods. In addition, by offloading modules, our method can achieve, at most, 75\% savings in execution time and 56\% in battery usage.},
  keywords = {Android,Batteries,cloud computing,computation offloading,coprocessors,Coprocessors,Energy consumption,Graphics processing units,Mobile communication,notion,Servers,Smart phones},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\NWQPMX52\\6675770.html}
}

@article{liuIncentiveMechanismComputation2017,
  title = {Incentive Mechanism for Computation Offloading Using Edge Computing: {{A Stackelberg}} Game Approach},
  shorttitle = {Incentive Mechanism for Computation Offloading Using Edge Computing},
  author = {Liu, Yang and Xu, Changqiao and Zhan, Yufeng and Liu, Zhixin and Guan, Jianfeng and Zhang, Hongke},
  year = {2017},
  month = dec,
  journal = {Computer Networks},
  series = {Special {{Issue}} on {{5G Wireless Networks}} for {{IoT}} and {{Body Sensors}}},
  volume = {129},
  pages = {399--409},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2017.03.015},
  urldate = {2023-03-01},
  abstract = {IoT-based services benefit from cloud which offers a virtually unlimited capabilities, such as storage, processing, and communication. However, the challenges are still open for mobile users to receive computation from the cloud with satisfied quality-of-service (QoS) provisioning. In this paper, we study computation offloading by using edge computing, which is a new paradigm to deliver computation to the edge of pervasive networks nearby mobile users. Without strong incentive in place, however, local edge servers may be reluctant to help offload computation. To stimulate cloud service operator and local edge server owners to participate in computation offloading, we formulate the interactions among cloud service operator and edge server owners as a Stackelberg game to maximize the utilities of cloud service operator and edge server owners by obtaining the optimal payment and computation offloading strategies. Through theoretical analysis, we show that the game is guaranteed to reach a unique Nash equilibrium. We then design two computation offloading algorithms that can quantify their efficiencies in terms of low delay and reduced complexity. Additionally, we extend our work by considering that edge server owners dynamically join or leave computation offloading. Numerical results show that our proposed algorithms perform well in computation offloading and efficiently stimulate edge server owners to make contribution to computation offloading.},
  langid = {english},
  keywords = {Computation offloading,Edge computing,Incentive mechanism,Nash equilibrium,notion,Stackelberg game},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\TISNXX26\\Liu et al. - 2017 - Incentive mechanism for computation offloading usi.pdf;C\:\\Users\\Wim\\Zotero\\storage\\NP6WHTZY\\S1389128617301068.html}
}

@article{liuMultiobjectiveOptimizationComputation2018,
  title = {Multiobjective {{Optimization}} for {{Computation Offloading}} in {{Fog Computing}}},
  author = {Liu, Liqing and Chang, Zheng and Guo, Xijuan and Mao, Shiwen and Ristaniemi, Tapani},
  year = {2018},
  month = feb,
  journal = {IEEE Internet of Things Journal},
  volume = {5},
  number = {1},
  pages = {283--294},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2017.2780236},
  abstract = {Fog computing system is an emergent architecture for providing computing, storage, control, and networking capabilities for realizing Internet of Things. In the fog computing system, the mobile devices (MDs) can offload its data or computational expensive tasks to the fog node within its proximity, instead of distant cloud. Although offloading can reduce energy consumption at the MDs, it may also incur a larger execution delay including transmission time between the MDs and the fog/cloud servers, and waiting and execution time at the servers. Therefore, how to balance the energy consumption and delay performance is of research importance. Moreover, based on the energy consumption and delay, how to design a cost model for the MDs to enjoy the fog and cloud services is also important. In this paper, we utilize queuing theory to bring a thorough study on the energy consumption, execution delay, and payment cost of offloading processes in a fog computing system. Specifically, three queuing models are applied, respectively, to the MD, fog, and cloud centers, and the data rate and power consumption of the wireless link are explicitly considered. Based on the theoretical analysis, a multiobjective optimization problem is formulated with a joint objective to minimize the energy consumption, execution delay, and payment cost by finding the optimal offloading probability and transmit power for each MD. Extensive simulation studies are conducted to demonstrate the effectiveness of the proposed scheme and the superior performance over several existed schemes are observed.},
  keywords = {Cloud computing,Computational modeling,cost,Delays,Edge computing,energy consumption,Energy consumption,execution delay,fog computing,Mobile communication,notion,offloading probability,Optimization,power allocation},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\RB263WYI\\Liu et al. - 2018 - Multiobjective Optimization for Computation Offloa.pdf;C\:\\Users\\Wim\\Zotero\\storage\\UXKCUNYD\\8168252.html}
}

@techreport{maagoeICTImpactStudy2020,
  type = {Technical Assistance},
  title = {{{ICT Impact}} Study},
  author = {Maag{\o}e, Viegand},
  year = {2020},
  month = jul,
  institution = {{Van Holsteijn en Kemna B.V.}},
  urldate = {2023-05-03},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\Q5A8RLSW\\IA_report-ICT_study_final_2020_(CIRCABC).pdf}
}

@article{machMobileEdgeComputing2017,
  title = {Mobile {{Edge Computing}}: {{A Survey}} on {{Architecture}} and {{Computation Offloading}}},
  shorttitle = {Mobile {{Edge Computing}}},
  author = {Mach, Pavel and Becvar, Zdenek},
  year = {2017},
  journal = {IEEE Communications Surveys \& Tutorials},
  volume = {19},
  number = {3},
  pages = {1628--1656},
  issn = {1553-877X},
  doi = {10.1109/COMST.2017.2682318},
  abstract = {Technological evolution of mobile user equipment (TIEs), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the TIEs is constrained by limited battery capacity and energy consumption of the TIEs. A suitable solution extending the battery life-time of the TIEs is to offload the applications demanding huge processing to a conventional centralized cloud. Nevertheless, this option introduces significant execution delay consisting of delivery of the off loaded applications to the cloud and back plus time of the computation at the cloud. Such a delay is inconvenient and makes the offloading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing (MEC), has been introduced. The MEC brings computation and storage resources to the edge of mobile network enabling it to run the highly demanding applications at the TIE while meeting strict delay requirements. The MEC computing resources can be exploited also by operators and third parties for specific purposes. In this paper, we first describe major use cases and reference scenarios where the MEC is applicable. After that we survey existing concepts integrating MEC functionalities to the mobile networks and discuss current advancement in standardization of the MEC. The core of this survey is, then, focused on user-oriented use case in the MEC, i.e., computation offloading. In this regard, we divide the research on computation offloading to three key areas: 1) decision on computation offloading; 2) allocation of computing resource within the MEC; and 3) mobility management. Finally, we highlight lessons learned in area of the MEC and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the MEC.},
  keywords = {allocation of computing resources,Batteries,Cloud computing,computation offloading,Delays,Edge computing,Mobile communication,Mobile computing,Mobile edge computing,mobile network architecture,mobility management,notion,standardization,Tutorials,use-cases},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\VNNCMJZT\\Mach and Becvar - 2017 - Mobile Edge Computing A Survey on Architecture an.pdf;C\:\\Users\\Wim\\Zotero\\storage\\5VWPVJEH\\7879258.html}
}

@misc{marcacciHowMuchEnergy2020,
  title = {How {{Much Energy Do Data Centers Really Use}}?},
  author = {Marcacci, Silvio},
  year = {2020},
  month = mar,
  journal = {Energy Innovation: Policy and Technology},
  urldate = {2023-05-02},
  abstract = {{$<$}p{$>$}This research review says that while data center energy usage is lower than estimated, clean energy technology must reduce their growing climate impact.{$<$}/p{$>$}},
  howpublished = {https://energyinnovation.org/2020/03/17/how-much-energy-do-data-centers-really-use/},
  langid = {american},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\SBVUZFYQ\\how-much-energy-do-data-centers-really-use.html}
}

@article{masanetRecalibratingGlobalData2020,
  title = {Recalibrating Global Data Center Energy-Use Estimates},
  author = {Masanet, Eric and Shehabi, Arman and Lei, Nuoa and Smith, Sarah and Koomey, Jonathan},
  year = {2020},
  month = feb,
  journal = {Science},
  volume = {367},
  number = {6481},
  pages = {984--986},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aba3758},
  urldate = {2023-05-04},
  abstract = {Growth in energy use has slowed owing to efficiency gains that smart policies can help maintain in the near term           ,                             Data centers represent the information backbone of an increasingly digitalized world. Demand for their services has been rising rapidly (               1               ), and data-intensive technologies such as artificial intelligence, smart and connected energy systems, distributed manufacturing systems, and autonomous vehicles promise to increase demand further (               2               ). Given that data centers are energy-intensive enterprises, estimated to account for around 1\% of worldwide electricity use, these trends have clear implications for global energy demand and must be analyzed rigorously. Several oft-cited yet simplistic analyses claim that the energy used by the world's data centers has doubled over the past decade and that their energy use will triple or even quadruple within the next decade (               3               \textendash{}               5               ). Such estimates contribute to a conventional wisdom (               5               ,               6               ) that as demand for data center services rises rapidly, so too must their global energy use. But such extrapolations based on recent service demand growth indicators overlook strong countervailing energy efficiency trends that have occurred in parallel (see the first figure). Here, we integrate new data from different sources that have emerged recently and suggest more modest growth in global data center energy use (see the second figure). This provides policy-makers and energy analysts a recalibrated understanding of global data center energy use, its drivers, and near-term efficiency potential.},
  langid = {english},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\JND2VWSP\\Masanet et al. - 2020 - Recalibrating global data center energy-use estima.pdf}
}

@article{masanetRecalibratingGlobalData2020a,
  title = {Recalibrating Global Data Center Energy-Use Estimates},
  author = {Masanet, Eric and Shehabi, Arman and Lei, Nuoa and Smith, Sarah and Koomey, Jonathan},
  year = {2020},
  month = feb,
  journal = {Science},
  volume = {367},
  number = {6481},
  pages = {984--986},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aba3758},
  urldate = {2023-05-02}
}

@article{masip-bruinManagingResourcesContinuity2018,
  title = {Managing Resources Continuity from the Edge to the Cloud: {{Architecture}} and Performance},
  shorttitle = {Managing Resources Continuity from the Edge to the Cloud},
  author = {{Masip-Bruin}, Xavi and {Marin-Tordera}, Eva and Jukan, Admela and Ren, Guang-Jie},
  year = {2018},
  month = feb,
  journal = {Future Generation Computer Systems},
  volume = {79},
  pages = {777--785},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.09.036},
  urldate = {2023-03-01},
  abstract = {The wide spread deployment of smart edge devices and applications that require real-time data processing, have with no doubt created the need to extend the reach of cloud computing to the edge, recently also referred to as Fog or Edge Computing. Fog computing implements the idea of extending the cloud where the ''things'' are, or in other words, improving application performance and resource efficiency by removing the need to processing all the information in the cloud, thus also reducing bandwidth consumption in the network. Fog computing is designed to complement cloud computing, paving the way for a novel, enriched architecture that can benefit from and include both edge(fog) and cloud resources. From a resources perspective, this combined scenario requires resource continuity when executing a service, whereby the assumption is that the selection of resources for service execution remains independent of their physical location. This new resources model, i.e., resource continuity, has gained recently significant attention, as it carries potential to seamlessly providing a computing infrastructure from the edge to the cloud, with an improved performance and resource efficiency. In this paper, we study the main architectural features of the managed resource continuity, proposing the foundation of a coordinated management plane responsible for resource continuity provisioning. We study an illustrative example on the performance benefits in relationship to the size of databases with regard to the proposed architectural model.},
  langid = {english},
  keywords = {Cloud computing,Edge computing,Fog computing,notion,Offloading,Resource management},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\ACDV6ZB9\\Masip-Bruin et al. - 2018 - Managing resources continuity from the edge to the.pdf;C\:\\Users\\Wim\\Zotero\\storage\\S892SUDF\\S0167739X17302686.html}
}

@inproceedings{meurischDecisionSupportComputational2017,
  title = {Decision {{Support}} for {{Computational Offloading}} by {{Probing Unknown Services}}},
  booktitle = {2017 26th {{International Conference}} on {{Computer Communication}} and {{Networks}} ({{ICCCN}})},
  author = {Meurisch, Christian and Gedeon, Julien and Nguyen, The An Binh and Kaup, Fabian and Muhlhauser, Max},
  year = {2017},
  month = jul,
  pages = {1--9},
  doi = {10.1109/ICCCN.2017.8038406},
  abstract = {Mobile Cloud Computing (MCC) leverages resourceful data centers that are distant (aka the cloud) or closely located (aka edge servers) for computational offloading to overcome resource limitations of modern mobile systems like smartphones or IoT devices. Many research works investigate context-aware offloading decision algorithms aiming to find the best offloading system at runtime. However, all approaches require prior knowledge of the offloading systems or a running service profiler on the backend system. In this paper, we present a novel approach that overcomes this issue by first probing available unknown services such as nearby cloudlets or the distant cloud, and networks in an energy-efficient way at runtime to make better offloading decisions. For that, we investigate a probing strategy to assess these unknown services by offloading micro tasks and accurately predicting the performance for larger offloading tasks using regression models. Our evaluation on three algorithms with different time complexities shows that we achieve high prediction accuracies up to 85.5\%, already after probing of two micro tasks running in the range of few milliseconds. To the best of our knowledge, this is the first supplement approach for offloading decision support that can handle unknown third-party services requiring no prior knowledge about these offloading systems and making no assumptions for real-world deployments.},
  keywords = {Batteries,Cloud computing,Mobile communication,Mobile computing,Mobile handsets,notion,Runtime,Servers},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\BHIBABY9\\8038406.html}
}

@misc{MicrosoftAzureIoT2021,
  title = {Microsoft {{Azure IoT Reference Architecture}}},
  year = {2021},
  month = apr,
  urldate = {2023-01-30},
  abstract = {The purpose of the document is to provide an overview of the recommended architecture and implementation technology choices for how to build Azure IoT solutions. This architecture describes terminology, technology principles, common configuration environments, and composition of Azure IoT serv...},
  howpublished = {https://azure.microsoft.com/en-us/resources/microsoft-azure-iot-reference-architecture/},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\54TXPWBW\\microsoft-azure-iot-reference-architecture.html}
}

@article{mocigembaSustainableComputing2006,
  title = {Sustainable {{Computing}}},
  author = {Mocigemba, Dennis},
  year = {2006},
  month = sep,
  journal = {Poiesis \& Praxis},
  volume = {4},
  pages = {163--184},
  doi = {10.1007/s10202-005-0018-8},
  abstract = {The term Sustainable Computing is used to transfer the political concept of sustainability to computer systems, including material components (hardware) as well as informational ones (software), development as well as consumption processes. Six dimensions of Sustainable Computing are being distinguished. Empirical discourses, initiatives and social movements within the IT industry are assigned to these dimensions. The introduced Sustainable Computing Concept serves as a classification system to better understand different discourses or debates within the IT world, partly historical, partly current. It allows to synthesize these discourses by emphasizing what they have in common: the aim to balance economic, social and ecological interests.},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\9SZPXSI8\\Mocigemba - 2006 - Sustainable Computing.pdf}
}

@article{mohrPredictingMachineLearning2021,
  title = {Predicting {{Machine Learning Pipeline Runtimes}} in the {{Context}} of {{Automated Machine Learning}}},
  author = {Mohr, Felix and Wever, Marcel and Tornede, Alexander and H{\"u}llermeier, Eyke},
  year = {2021},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {9},
  pages = {3055--3066},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3056950},
  abstract = {Automated machine learning (AutoML) seeks to automatically find so-called machine learning pipelines that maximize the prediction performance when being used to train a model on a given dataset. One of the main and yet open challenges in AutoMLis an effective use of computational resources: An AutoML process involves the evaluation of many candidate pipelines, which are costly but often ineffective because they are canceled due to a timeout. In this paper, we present an approach to predict the runtime of two-step machine learning pipelines with up to one pre-processor, which can be used to anticipate whether or not a pipeline will time out. Separate runtime models are trained offline for each algorithm that may be used in a pipeline, and an overall prediction is derived from these models. We empirically show that the approach increases successful evaluations made by an AutoML tool while preserving or even improving on the previously best solutions.},
  keywords = {Automated machine learning,hierarchical runtime prediction,Machine learning,Machine learning algorithms,notion,Pipelines,Prediction algorithms,Predictive models,Runtime,runtime prediction for classifiers and pipelines,Tools},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\EEIAZUC6\\9347828.html}
}

@misc{mokhtariSustainabilityPillarAWS2022,
  title = {Sustainability {{Pillar}} - {{AWS Well-Architected Framework}} - {{Sustainability Pillar}}},
  author = {Mokhtari, Sam and Sisson, Brendan and O'Toole, Margaret and Grunwald, Steffen and Eccles, Ryan and Lester, Rodney and Cockcroft, Adrian and Meyers, Ian and Carlson, Brian},
  year = {2022},
  month = dec,
  urldate = {2023-01-30},
  howpublished = {https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\JDQD8CFY\\sustainability-pillar.html}
}

@inproceedings{muniswamaiahGreenComputingInternet2020,
  title = {Green Computing for {{Internet}} of {{Things}}},
  booktitle = {2020 7th {{IEEE International Conference}} on {{Cyber Security}} and {{Cloud Computing}} ({{CSCloud}})/2020 6th {{IEEE International Conference}} on {{Edge Computing}} and {{Scalable Cloud}} ({{EdgeCom}})},
  author = {Muniswamaiah, Manoj and Agerwala, Tilak and Tappert, Charles C.},
  year = {2020},
  month = aug,
  pages = {182--185},
  doi = {10.1109/CSCloud-EdgeCom49738.2020.00039},
  abstract = {Cloud computing services are used to meet the ever-growing demand for IoT. Data centers are increasingly becoming one of the largest consumers of energy to provide the infrastructure for the IoT paradigm. The demand for energy increases in the future as more innovations emerge, and technology follows new practices resulting in green computing being adopted. Green computing strategies reduce energy consumption by IoT devices without degrading their performance. This paper will evaluate numerous aspects of green computing for IoT computing analyzing critical concepts, challenges, and remediation.},
  keywords = {cloud computing,Cloud computing,Conferences,Data centers,Ecosystems,edge computing,energy consumption,Energy consumption,Internet of Things (IoT),notion,Performance evaluation,Technological innovation}
}

@article{nadjarantoosiResourceProvisioningDataintensive2018,
  title = {Resource Provisioning for Data-Intensive Applications with Deadline Constraints on Hybrid Clouds Using {{Aneka}}},
  author = {Nadjaran Toosi, Adel and Sinnott, Richard O. and Buyya, Rajkumar},
  year = {2018},
  month = feb,
  journal = {Future Generation Computer Systems},
  volume = {79},
  pages = {765--775},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.05.042},
  urldate = {2023-03-03},
  abstract = {Cloud computing has emerged as a mainstream paradigm for hosting various types of applications by supporting easy-to-use computing services. Among the many different forms of cloud computing, hybrid clouds, which mix on-premises private cloud and third-party public cloud services to deploy applications, have gained broad acceptance. They are particularly relevant for applications requiring large volumes of computing power exceeding the computational capacity within the premises of a single organization. However, the use of hybrid clouds introduces the challenge of how much and when public cloud resources should be added to the pool of resources~\textendash ~and especially when it is necessary to support quality of service requirements of applications with deadline constraints. These resource provisioning decisions are far from trivial if scheduling involves data-intensive applications using voluminous amounts of data. Issues such as the impact of network latency, bandwidth constraints, and location of data must be taken into account in order to minimize the execution cost while meeting the deadline for such applications. In this paper, we propose a new resource provisioning algorithm to support the deadline requirements of data-intensive applications in hybrid cloud environments. To evaluate our proposed algorithm, we implement it in Aneka, a platform for developing scalable applications on the Cloud. Experimental results using a real case study executing a data-intensive application to measure the walkability index on a hybrid cloud platform consisting of dynamic resources from the Microsoft Azure cloud show that our proposed provisioning algorithm is able to more efficiently allocate resources compared to existing methods.},
  langid = {english},
  keywords = {Aneka cloud application platform,Data locality,Data-intensive applications,Deadline-driven scheduling,Dynamic provisioning,Hybrid cloud,Network bandwidth,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\UJY34ZSA\\Nadjaran Toosi et al. - 2018 - Resource provisioning for data-intensive applicati.pdf;C\:\\Users\\Wim\\Zotero\\storage\\SLVSNXTM\\S0167739X17301863.html}
}

@article{niuBandwidthadaptivePartitioningDistributed2014,
  title = {Bandwidth-Adaptive Partitioning for Distributed Execution Optimization of Mobile Applications},
  author = {Niu, Jianwei and Song, Wenfang and Atiquzzaman, Mohammed},
  year = {2014},
  month = jan,
  journal = {Journal of Network and Computer Applications},
  volume = {37},
  pages = {334--347},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2013.03.007},
  urldate = {2023-03-01},
  abstract = {Mobile applications are becoming increasingly ubiquitous and provide ever richer functionality on mobile devices, while such applications drain increasingly more battery power of mobile devices. Offloading some parts of the application running on mobile devices onto remote servers/clouds is a promising approach to extend the battery life of mobile devices. However, as data transmission of offloading causes delay and energy costs for mobile devices, it is necessary to carefully design application partitioning/offloading schemes to weigh the benefits against the transmission delay and costs. Due to bandwidth fluctuations in the wireless environment, static partitionings in previous work are unsuitable for mobile platforms with a fixed bandwidth assumption, while dynamic partitionings result in high overhead of continuous partitioning for mobile devices. Therefore, we propose a novel partitioning scheme taking the bandwidth as a variable to improve static partitioning and avoid high costs of dynamic partitioning. Firstly, we construct application Object Relation Graphs (ORGs) by combining static analysis and dynamic profiling to propose partitioning optimization models. Then based on our novel execution-time and energy optimization partitioning models, we propose the Branch-and-Bound based Application Partitioning (BBAP) algorithm and Min-Cut based Greedy Application Partitioning (MCGAP) algorithm. BBAP is suited to finding the optimal partitioning solutions for small applications, while MCGAP is applicable to quickly obtaining suboptimal solutions for large-scale applications. Experimental results demonstrate that both algorithms can adapt to bandwidth fluctuations well, and significantly reduce application execution time and energy consumption by optimally distributing components between mobile devices and servers.},
  langid = {english},
  keywords = {Application partitioning,Bandwidth-adaptive,Energy saving,Mobile applications,notion,Weighted object relation graphs},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\WDQ2KGT8\\S1084804513000945.html}
}

@misc{NvidiaGPU,
  title = {Nvidia {{GPU}}},
  journal = {Design Life-Cycle},
  urldate = {2023-05-05},
  howpublished = {http://www.designlife-cycle.com/nvidia-gpu},
  langid = {american},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\FD9SVRRN\\nvidia-gpu.html}
}

@misc{popMachineLearningCloud2016,
  title = {Machine {{Learning}} and {{Cloud Computing}}: {{Survey}} of {{Distributed}} and {{SaaS Solutions}}},
  shorttitle = {Machine {{Learning}} and {{Cloud Computing}}},
  author = {Pop, Daniel},
  year = {2016},
  month = mar,
  number = {arXiv:1603.08767},
  eprint = {1603.08767},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1603.08767},
  urldate = {2023-01-30},
  abstract = {Applying popular machine learning algorithms to large amounts of data raised new challenges for the ML practitioners. Traditional ML libraries does not support well processing of huge datasets, so that new approaches were needed. Parallelization using modern parallel computing frameworks, such as MapReduce, CUDA, or Dryad gained in popularity and acceptance, resulting in new ML libraries developed on top of these frameworks. We will briefly introduce the most prominent industrial and academic outcomes, such as Apache Mahout, GraphLab or Jubatus. We will investigate how cloud computing paradigm impacted the field of ML. First direction is of popular statistics tools and libraries (R system, Python) deployed in the cloud. A second line of products is augmenting existing tools with plugins that allow users to create a Hadoop cluster in the cloud and run jobs on it. Next on the list are libraries of distributed implementations for ML algorithms, and on-premise deployments of complex systems for data analytics and data mining. Last approach on the radar of this survey is ML as Software-as-a-Service, several BigData start-ups (and large companies as well) already opening their solutions to the market.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Machine Learning,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\WKR23YES\\Pop - 2016 - Machine Learning and Cloud Computing Survey of Di.pdf;C\:\\Users\\Wim\\Zotero\\storage\\QVJL4P46\\1603.html}
}

@article{quisbert-trujilloDesignMethodologySustainable,
  title = {Design Methodology for Sustainable {{IoT}} Systems},
  author = {{Quisbert-Trujillo}, Ernesto},
  langid = {english},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\DP7A3X7V\\Quisbert-Trujillo - Design methodology for sustainable IoT systems.pdf}
}

@article{ramirezEvaluatingBenefitsCombined2017,
  title = {Evaluating the Benefits of Combined and Continuous {{Fog-to-Cloud}} Architectures},
  author = {Ramirez, W. and {Masip-Bruin}, X. and {Marin-Tordera}, E. and Souza, V. B. C. and Jukan, A. and Ren, G-J. and {Gonzalez de Dios}, O.},
  year = {2017},
  month = nov,
  journal = {Computer Communications},
  volume = {113},
  pages = {43--52},
  issn = {0140-3664},
  doi = {10.1016/j.comcom.2017.09.011},
  urldate = {2023-03-01},
  abstract = {The need to extend the features of Cloud computing to the edge of the network has fueled the development of new computing architectures, such as Fog computing. When put together, the combined and continuous use of fog and cloud computing, lays the foundation for a new and highly heterogeneous computing ecosystem, making the most out of both, cloud and fog. Incipient research efforts are devoted to propose a management architecture to properly manage such combination of resources, such as the reference architecture proposed by the OpenFog Consortium or the recent Fog-to-Cloud (F2C). In this paper, we pay attention to such a combined ecosystem and particularly evaluate the potential benefits of F2C in dynamic scenarios, considering computing resources mobility and different traffic patterns. By means of extensive simulations we specifically study the aspects of service response time, network bandwidth occupancy, power consumption and service disruption probability. The results indicate that a combined fog-to-cloud architecture brings significant performance benefits in comparison with the traditional standalone Cloud, e.g., over 50\% reduction in terms of power consumption.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\7ZWFDX3R\\Ramirez et al. - 2017 - Evaluating the benefits of combined and continuous.pdf;C\:\\Users\\Wim\\Zotero\\storage\\V9ZSNRQ6\\S0140366417301792.html}
}

@inproceedings{regoDecisionTreeBasedApproaches2017,
  title = {Decision {{Tree-Based Approaches}} for {{Handling Offloading Decisions}} and {{Performing Adaptive Monitoring}} in {{MCC Systems}}},
  booktitle = {2017 5th {{IEEE International Conference}} on {{Mobile Cloud Computing}}, {{Services}}, and {{Engineering}} ({{MobileCloud}})},
  author = {Rego, Paulo A. L. and Cheong, Elaine and Coutinho, Emanuel F. and Trinta, Fernando A. M. and Hasan, Masum Z. and de Souza, Jose N.},
  year = {2017},
  month = apr,
  pages = {74--81},
  doi = {10.1109/MobileCloud.2017.19},
  abstract = {Mobile cloud computing (MCC) has emerged as a solution to overcome the resource constraints of mobile devices by using computation offloading to execute mobile application tasks on remote servers, thus enhancing performance and reducing the energy consumption of mobile devices. Nevertheless, the effectiveness of an offloading solution is determined by its ability to infer when offloading will improve performance. In this context, several solutions have been proposed to handle computational offloading operations and the decisions of when and where to offload. The problem is that such decisions depend on periodic monitoring of several metrics and usually involve compute intensive task that, when executed on mobile devices, can contribute to overhead the system. Thus, this paper proposes a novel approach for handling offloading decisions using decision trees and an adaptive monitoring scheme that allows MCC systems to monitor only the metrics that are relevant to the offloading decision. The results show that computation offloading can be beneficial for improving the performance of mobile applications and the energy consumption of mobile devices can be reduced by using the proposed adaptive monitoring scheme.},
  keywords = {adaptive monitoring,computation offloading,Computational modeling,decision trees,Decision trees,historical data,mobile cloud computing,Mobile handsets,Monitoring,notion,offloading decision,Performance evaluation,Servers},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\YH4T7PSE\\7944875.html}
}

@article{reinselDataAge20252017,
  title = {Data {{Age}} 2025: {{The Evolution}} of {{Data}} to {{Life-Critical}}},
  author = {Reinsel, David and Gantz, John and Rydning, John},
  year = {2017},
  month = apr,
  journal = {Don't Focus on Big Data},
  volume = {2},
  publisher = {{IDC Corporate Framingham, MA, USA}},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\INJ9KFUN\\IS-2018-93.pdf}
}

@article{reinselDigitizationWorldEdge2018,
  title = {The {{Digitization}} of the {{World}} from {{Edge}} to {{Core}}},
  author = {Reinsel, David and Gantz, John and Rydning, John},
  year = {2018},
  month = nov,
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\YEVH8U9V\\Reinsel et al. - 2018 - The Digitization of the World from Edge to Core.pdf}
}

@article{renServingEdgeScalable2017,
  title = {Serving at the {{Edge}}: {{A Scalable IoT Architecture Based}} on {{Transparent Computing}}},
  shorttitle = {Serving at the {{Edge}}},
  author = {Ren, Ju and Guo, Hui and Xu, Chugui and Zhang, Yaoxue},
  year = {2017},
  journal = {IEEE Network},
  volume = {31},
  number = {5},
  pages = {96--105},
  issn = {1558-156X},
  doi = {10.1109/MNET.2017.1700030},
  abstract = {By moving service provisioning from the cloud to the edge, edge computing becomes a promising solution in the era of IoT to meet the delay requirements of IoT applications, enhance the scalability and energy efficiency of lightweight IoT devices, provide contextual information processing, and mitigate the traffic burdens of the backbone network. However, as an emerging field of study, edge computing is still in its infancy and faces many challenges in its implementation and standardization. In this article, we study an implementation of edge computing, which exploits transparent computing to build scalable IoT platforms. Specifically, we first propose a transparent computing based IoT architecture, and clearly identify its advantages and associated challenges. Then, we present a case study to clearly show how to build scalable lightweight wearables with the proposed architecture. Some future directions are finally pointed out to foster continued research efforts.},
  keywords = {Cloud computing,Computer architecture,Edge computing,notion,Servers},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\PPW7VLLR\\Ren et al. - 2017 - Serving at the Edge A Scalable IoT Architecture B.pdf;C\:\\Users\\Wim\\Zotero\\storage\\UG9SILHH\\stamp.html}
}

@article{rydningDigitizationWorldEdge2018,
  title = {The Digitization of the World from Edge to Core},
  author = {Rydning, David Reinsel-John Gantz-John and Reinsel, J and Gantz, J},
  year = {2018},
  journal = {Framingham: International Data Corporation},
  volume = {16},
  keywords = {notion}
}

@article{sahaGreenComputing2014,
  title = {Green Computing},
  author = {Saha, Biswajit},
  year = {2014},
  journal = {International Journal of Computer Trends and Technology (IJCTT)},
  volume = {14},
  number = {2},
  pages = {46--50},
  keywords = {notion}
}

@article{sandobalinEffectivenessToolsSupport2020,
  title = {On the {{Effectiveness}} of {{Tools}} to {{Support Infrastructure}} as {{Code}}: {{Model-Driven Versus Code-Centric}}},
  shorttitle = {On the {{Effectiveness}} of {{Tools}} to {{Support Infrastructure}} as {{Code}}},
  author = {Sandobal{\'i}n, Julio and Insfran, Emilio and Abrah{\~a}o, Silvia},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {17734--17761},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2966597},
  abstract = {Infrastructure as Code (IaC) is an approach for infrastructure automation that is based on software development practices. The IaC approach supports code-centric tools that use scripts to specify the creation, updating and execution of cloud infrastructure resources. Since each cloud provider offers a different type of infrastructure, the definition of an infrastructure resource (e.g., a virtual machine) implies writing several lines of code that greatly depend on the target cloud provider. Model-driven tools, meanwhile, abstract the complexity of using IaC scripts through the high-level modeling of the cloud infrastructure. In a previous work, we presented an infrastructure modeling approach and tool (Argon) for cloud provisioning that leverages model-driven engineering and supports the IaC approach. The objective of the present work is to compare a model-driven tool (Argon) with a well-known code-centric tool (Ansible) in order to provide empirical evidence of their effectiveness when defining the cloud infrastructure, and the participants' perceptions when using these tools. We, therefore, conducted a family of three experiments involving 67 Computer Science students in order to compare Argon with Ansible as regards their effectiveness, efficiency, perceived ease of use, perceived usefulness, and intention to use. We used the AB/BA crossover design to configure the individual experiments and the linear mixed model to statistically analyze the data collected and subsequently obtain empirical findings. The results of the individual experiments and meta-analysis indicate that Argon is more effective as regards supporting the IaC approach in terms of defining the cloud infrastructure. The participants also perceived that Argon is easier to use and more useful for specifying the infrastructure resources. Our findings suggest that Argon accelerates the provisioning process by modeling the cloud infrastructure and automating the generation of scripts for different DevOps tools when compared to Ansible, which is a code-centric tool that is greatly used in practice.},
  keywords = {Argon,Automation,Cloud computing,Computational modeling,controlled experiments,crossover design,DevOps,Infrastructure as code,linear mixed model,model-driven engineering,notion,Software,Tools,Unified modeling language},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\GCW57C6P\\Sandobalín et al. - 2020 - On the Effectiveness of Tools to Support Infrastru.pdf;C\:\\Users\\Wim\\Zotero\\storage\\3MAAQZNI\\8959180.html}
}

@misc{santosHowDoesDocker2017,
  title = {How Does {{Docker}} Affect Energy Consumption? {{Evaluating}} Workloads in and out of {{Docker}} Containers},
  shorttitle = {How Does {{Docker}} Affect Energy Consumption?},
  author = {Santos, Eddie Antonio and McLean, Carson and Solinas, Christopher and Hindle, Abram},
  year = {2017},
  month = may,
  number = {arXiv:1705.01176},
  eprint = {1705.01176},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-01-30},
  abstract = {Context: Virtual machines provide isolation of services at the cost of hypervisors and more resource usage. This spurred the growth of systems like Docker that enable single hosts to isolate several applications, similar to VMs, within a low-overhead abstraction called containers. Motivation: Although containers tout low overhead performance, do they still have low energy consumption? Methodology: This work statistically compares (\$t\$-test, Wilcoxon) the energy consumption of three application workloads in Docker and on bare-metal Linux. Results: In all cases, there was a statistically significant (\$t\$-test and Wilcoxon \$p {$<$} 0.05\$) increase in energy consumption when running tests in Docker, mostly due to the performance of I/O system calls.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance,H.3.4,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\B7UFAEGR\\Santos et al. - 2017 - How does Docker affect energy consumption Evaluat.pdf;C\:\\Users\\Wim\\Zotero\\storage\\29Z6L9UJ\\1705.html}
}

@misc{SDFabric,
  title = {{{SD-Fabric}}},
  journal = {Open Networking Foundation},
  urldate = {2023-04-14},
  langid = {american},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\6453V7AV\\SD-Fabric.html}
}

@article{shiEdgeComputingAnEmerging2017,
  title = {Edge {{Computing-An Emerging Computing Model}} for the {{Internet}} of {{Everything Era}}},
  author = {Shi, W. and Sun, H. and Cao, J. and Zhang, Q. and Liu, W.},
  year = {2017},
  month = may,
  journal = {Jisuanji Yanjiu yu Fazhan/Computer Research and Development},
  volume = {54},
  pages = {907--924},
  doi = {10.7544/issn1000-1239.2017.20160941},
  abstract = {With the proliferation of Internet of things (IoT) and the burgeoning of 4G/5G network, we have seen the dawning of the IoE (Internet of everything) era, where there will be a huge volume of data generated by things that are immersed in our daily life, and hundreds of applications will be deployed at the edge to consume these data. Cloud computing as the de facto centralized big data processing platform is not efficient enough to support these applications emerging in IoE era, i.e., 1) the computing capacity available in the centralized cloud cannot keep up with the explosive growing computational needs of massive data generated at the edge of the network; 2) longer user-perceived latency caused by the data movement between the edge and the cloud; 3) privacy and security concerns from data owners in the edge; 4) energy constraints of edge devices. These issues in the centralized big data processing era have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Leveraging the power of cloud computing, edge computing has the potential to address the limitation of computing capability, the concerns of response time requirement, bandwidth cost saving, data safety and privacy, as well as battery life constraint. ``Edge'' in edge computing is defined as any computing and network resources along the path between data sources and cloud data centers. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
  keywords = {notion}
}

@article{shiEdgeComputingVision2016,
  title = {Edge {{Computing}}: {{Vision}} and {{Challenges}}},
  shorttitle = {Edge {{Computing}}},
  author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
  year = {2016},
  month = oct,
  journal = {IEEE Internet of Things Journal},
  volume = {3},
  number = {5},
  pages = {637--646},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2016.2579198},
  abstract = {The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
  keywords = {Bandwidth,Cloud computing,Data privacy,Edge computing,Internet of things,Internet of Things (IoT),Mobile handsets,notion,smart home and city,Smart homes,Time factors},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\SMQBFNBI\\7488250.html}
}

@article{terefeEnergyefficientMultisiteOffloading2016,
  title = {Energy-Efficient Multisite Offloading Policy Using {{Markov}} Decision Process for Mobile Cloud Computing},
  author = {Terefe, Mati B. and Lee, Heezin and Heo, Nojung and Fox, Geoffrey C. and Oh, Sangyoon},
  year = {2016},
  month = apr,
  journal = {Pervasive and Mobile Computing},
  volume = {27},
  pages = {75--89},
  issn = {1574-1192},
  doi = {10.1016/j.pmcj.2015.10.008},
  urldate = {2023-03-01},
  abstract = {Mobile systems, such as smartphones, are becoming the primary platform of choice for a user's computational needs. However, mobile devices still suffer from limited resources such as battery life and processor performance. To address these limitations, a popular approach used in mobile cloud computing is computation offloading, where resource-intensive mobile components are offloaded to more resourceful cloud servers. Prior studies in this area have focused on a form of offloading where only a single server is considered as the offloading site. Because there is now an environment where mobile devices can access multiple cloud providers, it is possible for mobiles to save more energy by offloading energy-intensive components to multiple cloud servers. The method proposed in this paper differentiates the data- and computation-intensive components of an application and performs a multisite offloading in a data and process-centric manner. In this paper, we present a novel model to describe the energy consumption of a multisite application execution and use a discrete time Markov chain (DTMC) to model fading wireless mobile channels. We adopt a Markov decision process (MDP) framework to formulate the multisite partitioning problem as a delay-constrained, least-cost shortest path problem on a state transition graph. Our proposed Energy-efficient Multisite Offloading Policy (EMOP) algorithm, built on a value iteration algorithm (VIA), finds the efficient solution to the multisite partitioning problem. Numerical simulations show that our algorithm considers the different capabilities of sites to distribute appropriate components such that there is a lower energy cost for data transfer from the mobile to the cloud. A multisite offloading execution using our proposed EMOP algorithm achieved a greater reduction on the energy consumption of mobiles when compared to a single site offloading execution.},
  langid = {english},
  keywords = {MDP,Mobile cloud,Multisite,notion,Offloading},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\H3PWFK4B\\Terefe et al. - 2016 - Energy-efficient multisite offloading policy using.pdf;C\:\\Users\\Wim\\Zotero\\storage\\9QJTRL72\\S1574119215001923.html}
}

@inproceedings{tovazziGEMAnalyticsCloudtoEdgeAIPowered2020,
  title = {{{GEM-Analytics}}: {{Cloud-to-Edge AI-Powered Energy Management}}},
  shorttitle = {{{GEM-Analytics}}},
  booktitle = {Economics of {{Grids}}, {{Clouds}}, {{Systems}}, and {{Services}}},
  author = {Tovazzi, Daniele and Faticanti, Francescomaria and Siracusa, Domenico and Peroni, Claudio and Cretti, Silvio and Gazzini, Tommaso},
  editor = {Djemame, Karim and Altmann, J{\"o}rn and Ba{\~n}ares, Jos{\'e} {\'A}ngel and {Agmon Ben-Yehuda}, Orna and Stankovski, Vlado and Tuffin, Bruno},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {57--66},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-63058-4\_5},
  abstract = {Energy analysis, forecasting and optimization methods play a fundamental role in managing Combine Heat and Power (CHP) systems for energy production, in order to find the most suitable operational point. Indeed, several industries owning such cogeneration systems can significantly reduce overall costs by applying diverse techniques to predict, in real-time, the optimal load of the system. However, this is a complex task that requires processing a large amount of information from multiple data sources (IoT sensors, smart meters and much more), and, in most of the cases, is manually carried out by the energy manager of the company owning the CHP. For this reason, resorting to machine learning methods and new advanced technologies such as fog computing can significantly ease and automate real-time analyses and predictions for energy management systems that deal with huge amounts of data. In this paper we present GEM-Analytics, a new platform that exploits fog computing to enable AI-based methods for energy analysis at the edge of the network. In particular, we present two use cases involving CHP plants that need for optimal strategies to reduce the overall energy supply costs. In all the case studies we show that our platform can improve the energy load predictions compared to baselines thus reducing the costs incurred by industrial customers.},
  isbn = {978-3-030-63058-4},
  langid = {english},
  keywords = {AI prediction,CHP,Energy analysis,Fog computing,notion}
}

@misc{TrendReportEdge2021,
  title = {Trend {{Report}}: {{The Edge Compute Imperative}}},
  year = {2021},
  publisher = {{Lumen}},
  keywords = {notion},
  file = {C\:\\Users\\yedema21\\Documents\\trend-report-the-edge-compute-imperative.pdf}
}

@misc{unepdtuGreenhouseGasEmissions2020,
  title = {Greenhouse Gas Emissions in the {{ICT}} Sector},
  author = {UNEP DTU},
  year = {2020},
  month = mar,
  keywords = {notion}
}

@article{vargheseNextGenerationCloud2018,
  title = {Next Generation Cloud Computing: {{New}} Trends and Research Directions},
  shorttitle = {Next Generation Cloud Computing},
  author = {Varghese, Blesson and Buyya, Rajkumar},
  year = {2018},
  month = feb,
  journal = {Future Generation Computer Systems},
  volume = {79},
  pages = {849--861},
  issn = {0167-739X},
  doi = {10.1016/j.future.2017.09.020},
  urldate = {2023-03-01},
  abstract = {The landscape of cloud computing has significantly changed over the last decade. Not only have more providers and service offerings crowded the space, but also cloud infrastructure that was traditionally limited to single provider data centers is now evolving. In this paper, we firstly discuss the changing cloud infrastructure and consider the use of infrastructure from multiple providers and the benefit of decentralising computing away from data centers. These trends have resulted in the need for a variety of new computing architectures that will be offered by future cloud infrastructure. These architectures are anticipated to impact areas, such as connecting people and devices, data-intensive computing, the service space and self-learning systems. Finally, we lay out a roadmap of challenges that will need to be addressed for realising the potential of next generation cloud systems.},
  langid = {english},
  keywords = {Cloud computing,Cloud security,Cloudlet,Fog computing,Multi-cloud,notion,Serverless computing},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\IV3VYTUB\\Varghese and Buyya - 2018 - Next generation cloud computing New trends and re.pdf;C\:\\Users\\Wim\\Zotero\\storage\\J2Y7N7VW\\S0167739X17302224.html}
}

@article{verbraekenSurveyDistributedMachine2020,
  title = {A {{Survey}} on {{Distributed Machine Learning}}},
  author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
  year = {2020},
  month = mar,
  journal = {ACM Computing Surveys},
  volume = {53},
  number = {2},
  pages = {30:1--30:33},
  issn = {0360-0300},
  doi = {10.1145/3377454},
  urldate = {2023-01-30},
  abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
  keywords = {Distributed machine learning,distributed systems,notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\WK95UCPD\\Verbraeken et al. - 2020 - A Survey on Distributed Machine Learning.pdf}
}

@techreport{walshCarbonBenefitsCloud2020,
  title = {The {{Carbon Benefits}} of {{Cloud Computing}}: A {{Study}} of the {{Microsoft Cloud}}},
  shorttitle = {The {{Carbon Benefits}} of {{Cloud Computing}}},
  author = {Walsh, Noelle},
  year = {2020},
  institution = {{Microsoft}},
  urldate = {2023-01-30},
  abstract = {A 2018 study finds that the Microsoft cloud is as much as 93 percent more energy efficient and as much as 98 percent more carbon efficient than on-premises solutions.},
  langid = {american},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\FGKMARBJ\\Study Carbon, energy efficiency benefits of the M.pdf;C\:\\Users\\Wim\\Zotero\\storage\\KMYMEHRJ\\details.html}
}

@misc{walshMicrosoftSustainabilityCalculator2020,
  title = {{Microsoft Sustainability Calculator helps enterprises analyze the carbon emissions of their IT infrastructure}},
  author = {Walsh, Noelle},
  year = {2020},
  month = jan,
  journal = {Azure Blog},
  urldate = {2023-01-30},
  abstract = {For more than a decade, Microsoft has been investing to reduce environmental impact while supporting the digital transformation of organizations around the world through cloud services.},
  langid = {dutch},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\2ZI3MJ5Y\\microsoft-sustainability-calculator-helps-enterprises-analyze-the-carbon-emissions-of-their-it-.html}
}

@article{wangComputationOffloadingScheme2004,
  title = {A Computation Offloading Scheme on Handheld Devices},
  author = {Wang, Cheng and Li, Zhiyuan},
  year = {2004},
  month = jun,
  journal = {Journal of Parallel and Distributed Computing},
  series = {{{YJPDC Special Issue}} on {{Middleware}}},
  volume = {64},
  number = {6},
  pages = {740--746},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2003.10.005},
  urldate = {2023-03-01},
  abstract = {In this paper, we present a computation offloading scheme on handheld devices. This scheme partitions an ordinary program into a client\textendash server distributed program, such that the client code runs on the handheld device and the server code runs on the server. Our partition analysis and program transformation guarantee correct distributed execution under all possible execution contexts. We give a polynomial time algorithm to find the optimal program partition for given program input data. We use an option-clustering approach to handle different program partitions for different program execution options. Experimental results show significant improvement of performance and energy consumption on an HP IPAQ handheld device through computation offloading.},
  langid = {english},
  keywords = {Data consistency,Distributed computing,Edge profiling,notion,Program transformation,Task partition,Wireless network},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\6NZWEKG8\\Wang and Li - 2004 - A computation offloading scheme on handheld device.pdf;C\:\\Users\\Wim\\Zotero\\storage\\2PSIBW6T\\S0743731503001965.html}
}

@article{wangEnergyDelayTradeoff2017,
  title = {Energy and {{Delay Tradeoff}} for {{Application Offloading}} in {{Mobile Cloud Computing}}},
  author = {Wang, Xiumin and Wang, Jin and Wang, Xin and Chen, Xiaoming},
  year = {2017},
  month = jun,
  journal = {IEEE Systems Journal},
  volume = {11},
  number = {2},
  pages = {858--867},
  issn = {1937-9234},
  doi = {10.1109/JSYST.2015.2466617},
  abstract = {Recent work shows that offloading a mobile application from mobile devices to cloud servers can significantly reduce the energy consumption of mobile devices, thus extending the lifetime of mobile devices. However, previous work only considers the energy saving of mobile devices while ignoring the execution delay of mobile applications. To reduce the energy consumption of mobile devices, one may offload as many mobile applications as possible. However, offloading to cloud servers may incur a large execution delay because of the waiting time at the servers or the communication delay from the mobile devices to the servers. Thus, to balance the tradeoff between energy consumption and execution delay of mobile applications, it is necessary to determine whether the mobile application should be offloaded to the cloud server or run locally at the mobile devices. In this paper, we first formulate a joint optimization problem, which minimizes both the energy consumption at the mobile devices and the execution delay of mobile applications. We prove that the proposed problem is NP-hard. For a special case with unlimited residual energy at the mobile device and the same amount of resources required by each mobile application, we present a polynomial-time optimal solution. We also propose an efficient heuristic algorithm to solve the general case of the problem. Finally, simulation results demonstrate the effectiveness of the proposed scheme.},
  keywords = {Algorithm design and analysis,Application offloading,Delays,Energy consumption,energy efficient,mobile cloud,Mobile communication,Mobile handsets,notion,Servers},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\J9NYXRDS\\7225153.html}
}

@article{wangENORMFrameworkEdge2020,
  title = {{{ENORM}}: {{A Framework For Edge NOde Resource Management}}},
  shorttitle = {{{ENORM}}},
  author = {Wang, Nan and Varghese, Blesson and Matthaiou, Michail and Nikolopoulos, Dimitrios S.},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Services Computing},
  volume = {13},
  number = {6},
  pages = {1086--1099},
  issn = {1939-1374},
  doi = {10.1109/TSC.2017.2753775},
  abstract = {Current computing techniques using the cloud as a centralised server will become untenable as billions of devices get connected to the Internet. This raises the need for fog computing, which leverages computing at the edge of the network on nodes, such as routers, base stations and switches, along with the cloud. However, to realise fog computing the challenge of managing edge nodes will need to be addressed. This paper is motivated to address the resource management challenge. We develop the first framework to manage edge nodes, namely the Edge NOde Resource Management (ENORM) framework. Mechanisms for provisioning and auto-scaling edge node resources are proposed. The feasibility of the framework is demonstrated on a Pok\'eMon Go-like online game use-case. The benefits of using ENORM are observed by reduced application latency between 20-80 percent and reduced data transfer and communication frequency between the edge node and the cloud by up to 95 percent. These results highlight the potential of fog computing for improving the quality of service and experience.},
  keywords = {Cloud computing,Computational modeling,Edge computing,edge nodes,Fog computing,notion,provisioning,Quality of service,resource management,Resource management,Scalability,scaling resources,Servers},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\B8DN2HCL\\Wang et al. - 2020 - ENORM A Framework For Edge NOde Resource Manageme.pdf;C\:\\Users\\Wim\\Zotero\\storage\\UMA699QH\\8039523.html}
}

@inproceedings{wangMeetingGreenComputing2008,
  title = {Meeting {{Green Computing Challenges}}},
  booktitle = {2008 10th {{Electronics Packaging Technology Conference}}},
  author = {Wang, David},
  year = {2008},
  month = dec,
  pages = {121--126},
  doi = {10.1109/EPTC.2008.4763421},
  abstract = {Green computing or sustainability is not simply the operational energy consumption of computing equipment. Green computing must take the product life cycle into consideration, from production to operation to recycling. This study focuses on the production and the operation phases of the product life cycle and demonstrates what actions will result in overall carbon footprint reduction for personal and business computing under various operational conditions and environments. Energy consumption related to end-of-life product recycling is not discussed in this study and is not believed to have a significant impact on the validity of the sustainability analysis based on production and operation energy consumption and carbon footprint.},
  keywords = {Central Processing Unit,Energy conservation,Energy consumption,High performance computing,Home appliances,notion,Performance analysis,Personal communication networks,Production,Recycling,Solid state circuits},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\53EINXVL\\4763421.html}
}

@article{wangMobileEdgeComputingPartial2016,
  title = {Mobile-{{Edge Computing}}: {{Partial Computation Offloading Using Dynamic Voltage Scaling}}},
  shorttitle = {Mobile-{{Edge Computing}}},
  author = {Wang, Yanting and Sheng, Min and Wang, Xijun and Wang, Liang and Li, Jiandong},
  year = {2016},
  month = oct,
  journal = {IEEE Transactions on Communications},
  volume = {64},
  number = {10},
  pages = {4268--4282},
  issn = {1558-0857},
  doi = {10.1109/TCOMM.2016.2599530},
  abstract = {The incorporation of dynamic voltage scaling technology into computation offloading offers more flexibilities for mobile edge computing. In this paper, we investigate partial computation offloading by jointly optimizing the computational speed of smart mobile device (SMD), transmit power of SMD, and offloading ratio with two system design objectives: energy consumption of SMD minimization (ECM) and latency of application execution minimization (LM). Considering the case that the SMD is served by a single cloud server, we formulate both the ECM problem and the LM problem as nonconvex problems. To tackle the ECM problem, we recast it as a convex one with the variable substitution technique and obtain its optimal solution. To address the nonconvex and nonsmooth LM problem, we propose a locally optimal algorithm with the univariate search technique. Furthermore, we extend the scenario to a multiple cloud servers system, where the SMD could offload its computation to a set of cloud servers. In this scenario, we obtain the optimal computation distribution among cloud servers in closed form for the ECM and LM problems. Finally, extensive simulations demonstrate that our proposed algorithms can significantly reduce the energy consumption and shorten the latency with respect to the existing offloading schemes.},
  keywords = {Cloud computing,collaboration between communication and computation resources,Computer architecture,dynamic voltage scaling,Electronic countermeasures,Energy consumption,mobile-edge computing,notion,Optimization,Partial computation offloading,Servers,Voltage control},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\IPW882B8\\7542156.html}
}

@techreport{WeNR2021Release2021,
  title = {{{WeNR}} 2021 | {{Release}} of the Public Report: Impact of {{Information Systems}}},
  shorttitle = {{{WeNR}} 2021 | {{Release}} of the Public Report},
  year = {2021},
  institution = {{INR}},
  urldate = {2023-01-30},
  abstract = {INR presents WeNR 2021, an assessment of the impact of information systems and the sustainable IT maturity of European organisations.},
  langid = {british},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\HZ6HM786\\wenr-2021-public-report.html}
}

@misc{WhatEdgeComputing,
  title = {What Edge Computing Means for Hardware Companies | {{McKinsey}}},
  urldate = {2023-03-01},
  howpublished = {https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/new-demand-new-markets-what-edge-computing-means-for-hardware-companies?hlkid=9c60010d1b394427a06c47feb2e3401c\&hctky=2618809\&hdpid=1dbd559b-1690-4c8b-bc99-35b7ed7279a2},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\GPWA57I3\\new-demand-new-markets-what-edge-computing-means-for-hardware-companies.html}
}

@misc{WhatRawMaterials,
  title = {What {{Raw Materials Are Used}} to {{Make Hardware}} in {{Computing Devices}}?},
  journal = {Engineering.com},
  urldate = {2023-05-05},
  abstract = {Over 50 of the world's 90 naturally occurring elements are used in over 8.5 billion computing devices},
  howpublished = {https://www.engineering.com/story/what-raw-materials-are-used-to-make-hardware-in-computing-devices},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\LR2PV4KQ\\what-raw-materials-are-used-to-make-hardware-in-computing-devices.html}
}

@article{williamsEnergyIntensityComputer2004,
  title = {Energy {{Intensity}} of {{Computer Manufacturing}}: {{Hybrid Assessment Combining Process}} and {{Economic Input}}-{{Output Methods}}},
  shorttitle = {Energy {{Intensity}} of {{Computer Manufacturing}}},
  author = {Williams, Eric},
  year = {2004},
  month = nov,
  journal = {Environmental Science \& Technology},
  volume = {38},
  number = {22},
  pages = {6166--6174},
  issn = {0013-936X, 1520-5851},
  doi = {10.1021/es035152j},
  urldate = {2023-05-05},
  langid = {english},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\4XHHIGIY\\Williams - 2004 - Energy Intensity of Computer Manufacturing Hybrid.pdf}
}

@misc{xuOnlineLearningOffloading2016,
  title = {Online {{Learning}} for {{Offloading}} and {{Autoscaling}} in {{Renewable-Powered Mobile Edge Computing}}},
  author = {Xu, Jie and Ren, Shaolei},
  year = {2016},
  month = sep,
  number = {arXiv:1609.05087},
  eprint = {1609.05087},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-01},
  abstract = {Mobile edge computing (a.k.a. fog computing) has recently emerged to enable \textbackslash emph\{in-situ\} processing of delay-sensitive applications at the edge of mobile networks. Providing grid power supply in support of mobile edge computing, however, is costly and even infeasible (in certain rugged or under-developed areas), thus mandating on-site renewable energy as a major or even sole power supply in increasingly many scenarios. Nonetheless, the high intermittency and unpredictability of renewable energy make it very challenging to deliver a high quality of service to users in renewable-powered mobile edge computing systems. In this paper, we address the challenge of incorporating renewables into mobile edge computing and propose an efficient reinforcement learning-based resource management algorithm, which learns on-the-fly the optimal policy of dynamic workload offloading (to centralized cloud) and edge server provisioning to minimize the long-term system cost (including both service delay and operational cost). Our online learning algorithm uses a decomposition of the (offline) value iteration and (online) reinforcement learning, thus achieving a significant improvement of learning rate and run-time performance when compared to standard reinforcement learning algorithms such as Q-learning.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\LXDGQ53Z\\Xu and Ren - 2016 - Online Learning for Offloading and Autoscaling in .pdf;C\:\\Users\\Wim\\Zotero\\storage\\M5ERMGZ2\\1609.html}
}

@inproceedings{yangJointComputationPartitioning2017,
  title = {Joint {{Computation Partitioning}} and {{Resource Allocation}} for {{Latency Sensitive Applications}} in {{Mobile Edge Clouds}}},
  booktitle = {2017 {{IEEE}} 10th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  author = {Yang, Lei and Liu, Bo and Cao, Jiannong and Sahni, Yuvraj and Wang, Zhenyu},
  year = {2017},
  month = jun,
  pages = {246--253},
  issn = {2159-6190},
  doi = {10.1109/CLOUD.2017.39},
  abstract = {The proliferation of mobile devices and ubiquitous access of the wireless network enables many new mobile applications such as augmented reality, mobile gaming and so on. As the applications are latency sensitive, researchers propose to off load the complex computations of these applications to the nearby mobile edge cloud, in order to reduce the latency. Existing works mostly consider the problem of partitioning the computations between the mobile device and the traditional cloud that has abundant resources. The proposed approaches can not be applied in the context of mobile edge cloud, because both the resources in the mobile edge cloud and the wireless access bandwidth to the edge cloud are constrained. In this paper, we study joint computation partitioning and resource allocation problem for latency sensitive applications in mobile edge clouds. The problem is novel in that we combine the computation partitioning and the two-dimensional resource allocations in both the computation resources and the network bandwidth. We develop a new and efficient method, namely Multi-Dimensional Search and Adjust (MDSA), to solve the problem. We compares MDSA with the classic list scheduling method and the Search Adjust algorithm via comprehensive simulations. The results show that MDSA outperforms the benchmark algorithms in terms of the overall application latency.},
  keywords = {Bandwidth,Cloud computing,computation partitioning,Computational modeling,latency sensitive applications,Mobile communication,mobile edge cloud,Mobile handsets,notion,Resource management,Servers},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\49RTIMTD\\8030595.html}
}

@article{youEnergyEfficientResourceAllocation2017,
  title = {Energy-{{Efficient Resource Allocation}} for {{Mobile-Edge Computation Offloading}}},
  author = {You, Changsheng and Huang, Kaibin and Chae, Hyukjin and Kim, Byoung-Hoon},
  year = {2017},
  month = mar,
  journal = {IEEE Transactions on Wireless Communications},
  volume = {16},
  number = {3},
  pages = {1397--1411},
  issn = {1558-2248},
  doi = {10.1109/TWC.2016.2633522},
  abstract = {Mobile-edge computation offloading (MECO) off-loads intensive mobile computation to clouds located at the edges of cellular networks. Thereby, MECO is envisioned as a promising technique for prolonging the battery lives and enhancing the computation capacities of mobiles. In this paper, we study resource allocation for a multiuser MECO system based on time-division multiple access (TDMA) and orthogonal frequency-division multiple access (OFDMA). First, for the TDMA MECO system with infinite or finite cloud computation capacity, the optimal resource allocation is formulated as a convex optimization problem for minimizing the weighted sum mobile energy consumption under the constraint on computation latency. The optimal policy is proved to have a threshold-based structure with respect to a derived offloading priority function, which yields priorities for users according to their channel gains and local computing energy consumption. As a result, users with priorities above and below a given threshold perform complete and minimum offloading, respectively. Moreover, for the cloud with finite capacity, a sub-optimal resource-allocation algorithm is proposed to reduce the computation complexity for computing the threshold. Next, we consider the OFDMA MECO system, for which the optimal resource allocation is formulated as a mixed-integer problem. To solve this challenging problem and characterize its policy structure, a low-complexity sub-optimal algorithm is proposed by transforming the OFDMA problem to its TDMA counterpart. The corresponding resource allocation is derived by defining an average offloading priority function and shown to have close-to-optimal performance in simulation.},
  keywords = {Cloud computing,Computational modeling,Energy consumption,energy-efficient computing,Mobile communication,mobile computation offloading,Mobile-edge computing,notion,resource allocation,Resource management,Time division multiple access,Wireless communication},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\DI55Q8SV\\7762913.html}
}

@article{zamporiSuggestionsUpdatingOrganisation2019,
  title = {Suggestions for Updating the {{Organisation Environmental Footprint}} ({{OEF}}) Method},
  author = {Zampori, Luca},
  year = {2019},
  month = mar,
  publisher = {{EMSA: European Maritime Safety Agency}},
  urldate = {2023-07-13},
  abstract = {The information and views set out in this report are those of the author(s) and do not necessarily reflect the official opinion of the Commission 6 This JRC technical report is a working document and does not modify Recommendation 2013/179/EU on the use of common methods to measure and communicate the life cycle environmental performance of products and organisations Acknowledgements This report i. [...] 8 This JRC technical report is a working document and does not modify Recommendation 2013/179/EU on the use of common methods to measure and communicate the life cycle environmental performance of products and organisations Terminology: shall, should, may The OEF method uses precise terminology to indicate the requirements, the recommendations and options that the user of the OEF method may choose. [...] OEFSRs help to shift the focus of the OEF study towards those aspects and parameters that matter the most, and hence contribute to increased relevance, reproducibility and consistency of the results by reducing costs versus a study based on the comprehensive requirements of the OEF method. [...] Supply chain \textendash{} It refers to all of the upstream and downstream activities associated with the operations of the user of the OEF method, including the use of sold products by consumers and the end of life treatment of sold products after consumer use. [...] The OEF Guide was developed as one of the building blocks of the Flagship initiative of the Europe 2020 Strategy \textendash{} ``A Resource-Efficient Europe.''7 The European Commission's 5 OJ L 124, 4.5.2013 6 COM/2013/0196 final 7 European Commission 2011: COM(2011) 571 final: Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee.},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\P9LZ55QR\\2702461.html}
}

@article{zamporiSuggestionsUpdatingProduct2019,
  title = {Suggestions for Updating the {{Product Environmental Footprint}} ({{PEF}}) Method},
  author = {Zampori, Luca and Pant, Rana},
  year = {2019},
  series = {Publications {{Office}} of the {{European Union}}: {{Luxembourg}}},
  issn = {1831-9424},
  doi = {10.2760/424613},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\LTX848Y2\\PEF_method.pdf}
}

@inproceedings{zhangFireworkBigData2016,
  title = {Firework: {{Big Data Sharing}} and {{Processing}} in {{Collaborative Edge Environment}}},
  shorttitle = {Firework},
  booktitle = {2016 {{Fourth IEEE Workshop}} on {{Hot Topics}} in {{Web Systems}} and {{Technologies}} ({{HotWeb}})},
  author = {Zhang, Quan and Zhang, Xiaohong and Zhang, Qingyang and Shi, Weisong and Zhong, Hong},
  year = {2016},
  month = oct,
  pages = {20--25},
  doi = {10.1109/HotWeb.2016.12},
  abstract = {Cloud computing, arguably, has become the de facto computing platform for the big data processing by researchers and practitioners for the last decade, and enabled different stakeholders to discover valuable information from large scale data. At the same time, in the decade, we have witnessed the fast growing deployment of billions of sensors and actuators in multiple applications domains, such as transportation, manufacturing, connected/wearable health care, smart city and so on, stimulating the emerging of Edge Computing (a.k.a., fog computing, cloudlet). However, data, as the core of both cloud computing and edge computing, is still owned by each stakeholder and rarely shared due to privacy concern and formidable cost of data transportation, which significantly limits Internet of Things (IoT) applications that need data input from multiple stakeholders (e.g., video analytics collects data from cameras owned by police department, transportation department, retailer stores, etc.). In this paper, we envision that in the era of IoT the demand of distributed big data sharing and processing applications will dramatically increase since the data producing and consuming are pushed to the edge of the network. Data processing in collaborative edge environment needs to fuse data owned by multiple stakeholders, while keeping the computation within stakeholders' data facilities. To attack this challenge, we propose a new computing paradigm, Firework, which is designed for big data processing in collaborative edge environment (CEE). Firework fuses geographically distributed data by creating virtual shared data views that are exposed to end users via predefined interfaces by data owners. The interfaces are provided in the form of a set of datasets and a set of functions, where the functions are privacy preserved and bound to the datasets. Firework targets to share data while ensuring data privacy and integrity for stakeholders. By pushing the data processing as close as to data sources, Firework also aims to avoid data movement from the edge of the network to the cloud and improve the response latency.},
  keywords = {notion},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\5I6TE5P5\\7785712.html}
}

@article{zhouAugmentationTechniquesMobile2018,
  title = {Augmentation {{Techniques}} for {{Mobile Cloud Computing}}: {{A Taxonomy}}, {{Survey}}, and {{Future Directions}}},
  shorttitle = {Augmentation {{Techniques}} for {{Mobile Cloud Computing}}},
  author = {Zhou, Bowen and Buyya, Rajkumar},
  year = {2018},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {51},
  number = {1},
  pages = {13:1--13:38},
  issn = {0360-0300},
  doi = {10.1145/3152397},
  urldate = {2023-03-01},
  abstract = {Despite the rapid growth of hardware capacity and popularity in mobile devices, limited resources in battery and processing capacity still lack the ability to meet increasing mobile users' demands. Both conventional techniques and emerging approaches are brought together to fill this gap between user demand and mobile devices' limited capabilities. Recent research has focused on enhancing the performance of mobile devices via augmentation techniques. Augmentation techniques for mobile cloud computing refer to the computing paradigms and solutions to outsource mobile device computation and storage to more powerful computing resources in order to enhance a mobile device's computing capability and energy efficiency (e.g., code offloading). Adopting augmentation techniques in the heterogeneous and intermittent mobile cloud computing environment creates new challenges for computation management, energy efficiency, and system reliability. In this article, we aim to provide a comprehensive taxonomy and survey of the existing techniques and frameworks for mobile cloud augmentation regarding both computation and storage. Different from the existing taxonomies in this field, we focus on the techniques aspect, following the idea of realizing a complete mobile cloud computing system. The objective of this survey is to provide a guide on what available augmentation techniques can be adopted in mobile cloud computing systems as well as supporting mechanisms such as decision-making and fault tolerance policies for realizing reliable mobile cloud services. We also present a discussion on the open challenges and future research directions in this field.},
  keywords = {Mobile cloud computing,mobile device augmentation technique,notion}
}

@article{zhouResourceAllocationInformationCentric2017,
  title = {Resource {{Allocation}} for {{Information-Centric Virtualized Heterogeneous Networks With In-Network Caching}} and {{Mobile Edge Computing}}},
  author = {Zhou, Yuchen and Yu, F. Richard and Chen, Jian and Kuo, Yonghong},
  year = {2017},
  month = dec,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {66},
  number = {12},
  pages = {11339--11351},
  issn = {1939-9359},
  doi = {10.1109/TVT.2017.2737028},
  abstract = {In order to better accommodate the dramatically increasing demand for data caching and computing services, storage and computation capabilities should be endowed to some of the intermediate nodes within the network, therefore increasing the data throughput and reducing the network operation cost. In this paper, we design a novel information-centric heterogeneous networks framework aiming at enabling content caching and computing. Furthermore, due to the virtualization of the whole system, communication, computing, and caching resources can be shared among all users associated with different virtual service providers. We formulate the virtual resource allocation strategy as a joint optimization problem, where the gains of not only virtualization but also caching and computing are taken into consideration in the proposed information-centric heterogeneous networks virtualization architecture. In addition, a distributed algorithm based on alternating direction method of multipliers is adopted in order to solve the formulated problem. Since each base station only needs to solve its own problem without exchange of channel state information by using the distributed algorithm, the computational complexity and signaling overhead can be greatly reduced. Finally, extensive simulations are presented to show the effectiveness of the proposed scheme under different system parameters.},
  keywords = {Edge computing,Heterogeneous networks,in-network caching,Information-centric networking,Mobile communication,Mobile computing,mobile edge computing,Mobile handsets,notion,resource allocation,virtualization,Virtualization,Wireless networks},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\PMFZLL7R\\8003452.html}
}

@article{zhouResourceAllocationInformationCentric2017a,
  title = {Resource {{Allocation}} for {{Information-Centric Virtualized Heterogeneous Networks With In-Network Caching}} and {{Mobile Edge Computing}}},
  author = {Zhou, Yuchen and Yu, F. Richard and Chen, Jian and Kuo, Yonghong},
  year = {2017},
  month = dec,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {66},
  number = {12},
  pages = {11339--11351},
  issn = {1939-9359},
  doi = {10.1109/TVT.2017.2737028},
  abstract = {In order to better accommodate the dramatically increasing demand for data caching and computing services, storage and computation capabilities should be endowed to some of the intermediate nodes within the network, therefore increasing the data throughput and reducing the network operation cost. In this paper, we design a novel information-centric heterogeneous networks framework aiming at enabling content caching and computing. Furthermore, due to the virtualization of the whole system, communication, computing, and caching resources can be shared among all users associated with different virtual service providers. We formulate the virtual resource allocation strategy as a joint optimization problem, where the gains of not only virtualization but also caching and computing are taken into consideration in the proposed information-centric heterogeneous networks virtualization architecture. In addition, a distributed algorithm based on alternating direction method of multipliers is adopted in order to solve the formulated problem. Since each base station only needs to solve its own problem without exchange of channel state information by using the distributed algorithm, the computational complexity and signaling overhead can be greatly reduced. Finally, extensive simulations are presented to show the effectiveness of the proposed scheme under different system parameters.},
  keywords = {Edge computing,Heterogeneous networks,in-network caching,Information-centric networking,Mobile communication,Mobile computing,mobile edge computing,Mobile handsets,notion,resource allocation,virtualization,Virtualization,Wireless networks},
  file = {C\:\\Users\\Wim\\Zotero\\storage\\RI2ZMVS3\\8003452.html}
}
